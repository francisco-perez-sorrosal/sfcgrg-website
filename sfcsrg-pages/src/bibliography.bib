@book{clark2023,
	title = {The Experience Machine: },
	url = {https://www.amazon.com/Experience-Machine-Minds-Predict-Reality/dp/1524748455},
	publisher = {},
	author = {Andy Clark},
	date = {2023},
	langid = {english},
    abstract = {Widely acclaimed philosopher and cognitive scientist Andy Clark unpacks this provocative new theory that the brain is a powerful, dynamic prediction engine, mediating our experience of both body and world. From the most mundane experiences to the most sublime, reality as we know it is the complex synthesis of sensory information and expectation. Exploring its fascinating mechanics and remarkable implications for our lives, mental health, and society, Clark nimbly illustrates how the predictive brain sculpts all human experience. Chronic pain and mental illness are shown to involve subtle malfunctions of our unconscious predictions, pointing the way towards more effective, targeted treatments. Under renewed scrutiny, the very boundary between ourselves and the outside world dissolves, showing that we are as entangled with our environments as we are with our onboard memories, thoughts, and feelings. And perception itself is revealed to be something of a controlled hallucination.
Unveiling the extraordinary explanatory power of the predictive brain, The Experience Machine is a mesmerizing window onto one of the most significant developments in our understanding of the mind.}
}

@book{seth2021,
	title = {Being You: A New Science of Consciousness},
	url = {https://www.amazon.com/Being-You-New-Science-Consciousness/dp/1524742872},
	publisher = {},
	author = {Anil Seth},
	date = {2021},
	langid = {english},
    abstract = {Anil Seth's quest to understand the biological basis of conscious experience is one of the most exciting contributions to twenty-first-century science.
What does it mean to “be you”—that is, to have a specific, conscious experience of the world around you and yourself within it? There may be no more elusive or fascinating question. Historically, humanity has considered the nature of consciousness to be a primarily spiritual or philosophical inquiry, but scientific research is now mapping out compelling biological theories and explanations for consciousness and selfhood.
Now, internationally renowned neuroscience professor, researcher, and author Anil Seth is offers a window into our consciousness in BEING YOU: A New Science of Consciousness. Anil Seth is both a leading expert on the neuroscience of consciousness and one of most prominent spokespeople for this relatively new field of science. His radical argument is that we do not perceive the world as it objectively is, but rather that we are prediction machines, constantly inventing our world and correcting our mistakes by the microsecond, and that we can now observe the biological mechanisms in the brain that accomplish this process of consciousness.
Seth has been interviewed for documentaries aired on the BBC, Netflix, and Amazon and podcasts by Sam Harris, Russell Brand, and Chris Anderson, and his 2017 TED Talk on the topic has been viewed over 11 million times, a testament to his uncanny ability to make unimaginably complex science accessible and entertaining.}
}

@book{barret2020,
	title = {Seven and a Half Lessons About The Brain},
	url = {https://www.amazon.com/Seven-Half-Lessons-About-Brain/dp/035864559X},
	publisher = {},
	author = {Lisa Feldman Barrett},
	date = {2020},
	langid = {english},
    abstract = {A recent and concise primer on the brain.}
}

@book{tverskyb2019,
	title = {Mind in Motion: How Action Shapes Thought},
	url = {https://www.amazon.com/Mind-in-Motion-Barbara-Tversky-audiobook/dp/B07WD573XS},
	publisher = {},
	author = {Barbara Tversky},
	date = {2019},
	langid = {english},
    abstract = {When we try to think about how we think, we can't help but think of words. Indeed, some have called language the stuff of thought. But pictures are remembered far better than words, and describing faces, scenes, and events defies words. Anytime you take a shortcut or play chess or basketball or rearrange your furniture in your mind, you've done something remarkable: abstract thinking without words. 
In Mind in Motion, psychologist Barbara Tversky shows that spatial cognition isn't just a peripheral aspect of thought, but its very foundation, enabling us to draw meaning from our bodies and their actions in the world. Our actions in real space get turned into mental actions on thought, often spouting spontaneously from our bodies as gestures. Spatial thinking underlies creating and using maps, assembling furniture, devising football strategies, designing airports, understanding the flow of people, traffic, water, and ideas. Spatial thinking even underlies the structure and meaning of language: why we say we push ideas forward or tear them apart, why we're feeling up or have grown far apart. 
Like Thinking, Fast and Slow before it, Mind in Motion gives us a new way to think about how - and where - thinking takes place.}
}

@article{schrimpf_neural_2021,
	title = {The neural architecture of language: Integrative modeling converges on predictive processing},
	volume = {118},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2105646118},
	doi = {10.1073/pnas.2105646118},
	shorttitle = {The neural architecture of language},
	abstract = {Significance
            Language is a quintessentially human ability. Research has long probed the functional architecture of language in the mind and brain using diverse neuroimaging, behavioral, and computational modeling approaches. However, adequate neurally-mechanistic accounts of how meaning might be extracted from language are sorely lacking. Here, we report a first step toward addressing this gap by connecting recent artificial neural networks from machine learning to human recordings during language processing. We find that the most powerful models predict neural and behavioral responses across different datasets up to noise levels. Models that perform better at predicting the next word in a sequence also better predict brain measurements—providing computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the brain.
          , 
            The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species’ signature cognitive skill. We find that the most powerful “transformer” models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional {MRI} and electrocorticography). Models’ neural fits (“brain score”) and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.},
	pages = {e2105646118},
	number = {45},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	urldate = {2023-10-29},
	date = {2021-11-09},
	langid = {english},
}

@misc{lake_building_2016,
	title = {Building Machines That Learn and Think Like People},
	url = {http://arxiv.org/abs/1604.00289},
	doi = {10.48550/arXiv.1604.00289},
	abstract = {Recent progress in artificial intelligence ({AI}) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	number = {{arXiv}:1604.00289},
	publisher = {{arXiv}},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	urldate = {2023-10-29},
	date = {2016-11-02},
	eprinttype = {arxiv},
	eprint = {1604.00289 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{clark_whatever_2013,
	title = {Whatever next? Predictive brains, situated agents, and the future of cognitive science},
	volume = {36},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X12000477/type/journal_article},
	doi = {10.1017/S0140525X12000477},
	shorttitle = {Whatever next?},
	abstract = {Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this “hierarchical prediction machine” approach, concluding that it offers the best clue yet to the shape of a uniﬁed science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.},
	pages = {181--204},
	number = {3},
	journaltitle = {Behavioral and Brain Sciences},
	shortjournal = {Behav Brain Sci},
	author = {Clark, Andy},
	urldate = {2023-09-23},
	date = {2013-06},
	langid = {english},
}
