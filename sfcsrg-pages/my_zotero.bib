
@article{agarwal_neural_2020,
	title = {Neural Additive Models: Interpretable Machine Learning with Neural Nets},
	url = {http://arxiv.org/abs/2004.13912},
	shorttitle = {Neural Additive Models},
	abstract = {Deep neural networks ({DNNs}) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models ({NAMs}) which combine some of the expressivity of {DNNs} with the inherent intelligibility of generalized additive models. {NAMs} learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that {NAMs} are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but can be more easily applied to real-world problems.},
	journaltitle = {{arXiv}:2004.13912 [cs, stat]},
	author = {Agarwal, Rishabh and Frosst, Nicholas and Zhang, Xuezhou and Caruana, Rich and Hinton, Geoffrey E.},
	urldate = {2021-04-14},
	date = {2020-04-28},
	eprinttype = {arxiv},
	eprint = {2004.13912},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{akama_elements_2015,
	location = {Cham},
	title = {Elements of Quantum Computing},
	isbn = {978-3-319-08283-7 978-3-319-08284-4},
	url = {http://link.springer.com/10.1007/978-3-319-08284-4},
	publisher = {Springer International Publishing},
	author = {Akama, Seiki},
	urldate = {2021-02-15},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-08284-4},
}

@article{al-rfou_character-level_2018,
	title = {Character-Level Language Modeling with Deeper Self-Attention},
	url = {http://arxiv.org/abs/1808.04444},
	abstract = {{LSTMs} and other {RNN} variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms {RNN} variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.},
	journaltitle = {{arXiv}:1808.04444 [cs, stat]},
	author = {Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
	urldate = {2019-10-22},
	date = {2018-08-09},
	eprinttype = {arxiv},
	eprint = {1808.04444},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{alyafeai_survey_2020,
	title = {A Survey on Transfer Learning in Natural Language Processing},
	url = {http://arxiv.org/abs/2007.04239},
	abstract = {Deep learning models usually require a huge amount of data. However, these large datasets are not always attainable. This is common in many challenging {NLP} tasks. Consider Neural Machine Translation, for instance, where curating such large datasets may not be possible specially for low resource languages. Another limitation of deep learning models is the demand for huge computing resources. These obstacles motivate research to question the possibility of knowledge transfer using large trained models. The demand for transfer learning is increasing as many large models are emerging. In this survey, we feature the recent transfer learning advances in the field of {NLP}. We also provide a taxonomy for categorizing different transfer learning approaches from the literature.},
	journaltitle = {{arXiv}:2007.04239 [cs, stat]},
	author = {Alyafeai, Zaid and {AlShaibani}, Maged Saeed and Ahmad, Irfan},
	urldate = {2020-10-15},
	date = {2020-05-31},
	eprinttype = {arxiv},
	eprint = {2007.04239},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{arute_quantum_2019,
	title = {Quantum supremacy using a programmable superconducting processor},
	volume = {574},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1666-5},
	doi = {10.1038/s41586-019-1666-5},
	pages = {505--510},
	number = {7779},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Arute, Frank and Arya, Kunal and Babbush, Ryan and Bacon, Dave and Bardin, Joseph C. and Barends, Rami and Biswas, Rupak and Boixo, Sergio and Brandao, Fernando G. S. L. and Buell, David A. and Burkett, Brian and Chen, Yu and Chen, Zijun and Chiaro, Ben and Collins, Roberto and Courtney, William and Dunsworth, Andrew and Farhi, Edward and Foxen, Brooks and Fowler, Austin and Gidney, Craig and Giustina, Marissa and Graff, Rob and Guerin, Keith and Habegger, Steve and Harrigan, Matthew P. and Hartmann, Michael J. and Ho, Alan and Hoffmann, Markus and Huang, Trent and Humble, Travis S. and Isakov, Sergei V. and Jeffrey, Evan and Jiang, Zhang and Kafri, Dvir and Kechedzhi, Kostyantyn and Kelly, Julian and Klimov, Paul V. and Knysh, Sergey and Korotkov, Alexander and Kostritsa, Fedor and Landhuis, David and Lindmark, Mike and Lucero, Erik and Lyakh, Dmitry and Mandrà, Salvatore and {McClean}, Jarrod R. and {McEwen}, Matthew and Megrant, Anthony and Mi, Xiao and Michielsen, Kristel and Mohseni, Masoud and Mutus, Josh and Naaman, Ofer and Neeley, Matthew and Neill, Charles and Niu, Murphy Yuezhen and Ostby, Eric and Petukhov, Andre and Platt, John C. and Quintana, Chris and Rieffel, Eleanor G. and Roushan, Pedram and Rubin, Nicholas C. and Sank, Daniel and Satzinger, Kevin J. and Smelyanskiy, Vadim and Sung, Kevin J. and Trevithick, Matthew D. and Vainsencher, Amit and Villalonga, Benjamin and White, Theodore and Yao, Z. Jamie and Yeh, Ping and Zalcman, Adam and Neven, Hartmut and Martinis, John M.},
	urldate = {2019-12-06},
	date = {2019-10},
	langid = {english},
}

@article{ba_layer_2016,
	title = {Layer Normalization},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	journaltitle = {{arXiv}:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	urldate = {2020-03-06},
	date = {2016-07-21},
	eprinttype = {arxiv},
	eprint = {1607.06450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{baevski_data2vec_nodate,
	title = {data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language},
	abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, {NLP} or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/ tree/master/examples/data2vec.},
	pages = {13},
	author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
	langid = {english},
}

@book{barrett_seven_2021,
	title = {Seven and a Half Lessons About the Brain},
	isbn = {978-0-358-64559-7},
	abstract = {From the author of How Emotions Are Made, a myth-busting primer on the brain in the tradition of Seven Brief Lessons on Physics and Astrophysics for People in a Hurry Have you ever wondered why you have a brain? Let renowned neuroscientist Lisa Feldman Barrett demystify that big gray blob between your ears. In seven short essays (plus a bite-size story about how brains evolved), this slim, entertaining, and accessible collection reveals mind-expanding lessons from the front lines of neuroscience research. You’ll learn where brains came from, how they’re structured (and why it matters), and how yours works in tandem with other brains to create everything you experience. Along the way, you’ll also learn to dismiss popular myths such as the idea of a “lizard brain” and the alleged battle between thoughts and emotions—or between nature and nurture—to determine your behavior.   Sure to intrigue casual readers and scientific veterans alike, Seven and a Half Lessons About the Brain is full of surprises, humor, and important implications for human nature—a gift of a book that you will want to savor again and again.},
	pagetotal = {192},
	publisher = {Mariner Books},
	author = {Barrett, Lisa},
	date = {2021-10-26},
}

@article{bengio_neural_2003,
	title = {A neural probabilistic language model},
	volume = {3},
	issn = {1532-4435},
	abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
	pages = {1137--1155},
	issue = {null},
	journaltitle = {The Journal of Machine Learning Research},
	shortjournal = {J. Mach. Learn. Res.},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Janvin, Christian},
	date = {2003-03-01},
}

@article{besold_neural-symbolic_2017,
	title = {Neural-Symbolic Learning and Reasoning: A Survey and Interpretation},
	url = {http://arxiv.org/abs/1711.03902},
	shorttitle = {Neural-Symbolic Learning and Reasoning},
	abstract = {The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.},
	journaltitle = {{arXiv}:1711.03902 [cs]},
	author = {Besold, Tarek R. and Garcez, Artur d'Avila and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Kuehnberger, Kai-Uwe and Lamb, Luis C. and Lowd, Daniel and Lima, Priscila Machado Vieira and de Penning, Leo and Pinkas, Gadi and Poon, Hoifung and Zaverucha, Gerson},
	urldate = {2021-04-15},
	date = {2017-11-10},
	eprinttype = {arxiv},
	eprint = {1711.03902},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{biesialska_continual_2020,
	title = {Continual Lifelong Learning in Natural Language Processing: A Survey},
	url = {http://arxiv.org/abs/2012.09823},
	doi = {10.18653/v1/2020.coling-main.574},
	shorttitle = {Continual Lifelong Learning in Natural Language Processing},
	abstract = {Continual learning ({CL}) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, {CL} is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of {CL} through the lens of various {NLP} tasks. Our survey discusses major challenges in {CL} and current methods applied in neural network models. We also provide a critical review of the existing {CL} evaluation methods and datasets in {NLP}. Finally, we present our outlook on future research directions.},
	pages = {6523--6541},
	journaltitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	author = {Biesialska, Magdalena and Biesialska, Katarzyna and Costa-jussà, Marta R.},
	urldate = {2021-04-09},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2012.09823},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{blank_quantum_2020,
	title = {Quantum classifier with tailored quantum kernel},
	volume = {6},
	rights = {2020 The Author(s)},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-020-0272-6},
	doi = {10.1038/s41534-020-0272-6},
	abstract = {Kernel methods have a wide spectrum of applications in machine learning. Recently, a link between quantum computing and kernel theory has been formally established, opening up opportunities for quantum techniques to enhance various existing machine-learning methods. We present a distance-based quantum classifier whose kernel is based on the quantum state fidelity between training and test data. The quantum kernel can be tailored systematically with a quantum circuit to raise the kernel to an arbitrary power and to assign arbitrary weights to each training data. Given a specific input state, our protocol calculates the weighted power sum of fidelities of quantum data in quantum parallel via a swap-test circuit followed by two single-qubit measurements, requiring only a constant number of repetitions regardless of the number of data. We also show that our classifier is equivalent to measuring the expectation value of a Helstrom operator, from which the well-known optimal quantum state discrimination can be derived. We demonstrate the performance of our classifier via classical simulations with a realistic noise model and proof-of-principle experiments using the {IBM} quantum cloud platform.},
	pages = {1--7},
	number = {1},
	journaltitle = {npj Quantum Information},
	author = {Blank, Carsten and Park, Daniel K. and Rhee, June-Koo Kevin and Petruccione, Francesco},
	urldate = {2020-11-07},
	date = {2020-05},
	langid = {english},
}

@article{bommasani_opportunities_2021,
	title = {On the Opportunities and Risks of Foundation Models},
	url = {http://arxiv.org/abs/2108.07258},
	abstract = {{AI} is undergoing a paradigm shift with the rise of models (e.g., {BERT}, {DALL}-E, {GPT}-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	journaltitle = {{arXiv}:2108.07258 [cs]},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Kohd, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	urldate = {2021-09-15},
	date = {2021-08-18},
	eprinttype = {arxiv},
	eprint = {2108.07258},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{bowman_eight_2023,
	title = {Eight Things to Know about Large Language Models},
	url = {http://arxiv.org/abs/2304.00612},
	abstract = {The widespread public deployment of large language models ({LLMs}) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: 1. {LLMs} predictably get more capable with increasing investment, even without targeted innovation. 2. Many important {LLM} behaviors emerge unpredictably as a byproduct of increasing investment. 3. {LLMs} often appear to learn and use representations of the outside world. 4. There are no reliable techniques for steering the behavior of {LLMs}. 5. Experts are not yet able to interpret the inner workings of {LLMs}. 6. Human performance on a task isn't an upper bound on {LLM} performance. 7. {LLMs} need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with {LLMs} are often misleading.},
	number = {{arXiv}:2304.00612},
	publisher = {{arXiv}},
	author = {Bowman, Samuel R.},
	urldate = {2023-05-02},
	date = {2023-04-02},
	eprinttype = {arxiv},
	eprint = {2304.00612 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{bowman_large_2015,
	location = {Lisbon, Portugal},
	title = {A large annotated corpus for learning natural language inference},
	url = {http://aclweb.org/anthology/D15-1075},
	doi = {10.18653/v1/D15-1075},
	abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classiﬁers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the ﬁrst time.},
	eventtitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	pages = {632--642},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	urldate = {2021-05-21},
	date = {2015},
	langid = {english},
}

@article{brittain_prioritized_nodate,
	title = {Prioritized Sequence Experience Replay},
	abstract = {Experience replay is widely used in deep reinforcement learning algorithms and allows agents to remember and learn from experiences from the past. In an effort to learn more efﬁciently, researchers proposed prioritized experience replay ({PER}) which samples important transitions more frequently. In this paper, we propose Prioritized Sequence Experience Replay ({PSER}) a framework for prioritizing sequences of experience in an attempt to both learn more efﬁciently and to obtain better performance. We compare the performance of {PER} and {PSER} sampling techniques in a tabular Q-learning environment and in {DQN} on the Atari 2600 benchmark. We prove theoretically that {PSER} is guaranteed to converge faster than {PER} and empirically show {PSER} substantially improves upon {PER}.},
	pages = {18},
	author = {Brittain, Marc and Bertram, Josh and Yang, Xuxi and Wei, Peng},
	langid = {english},
}

@article{broughton_tensorflow_2020,
	title = {{TensorFlow} Quantum: A Software Framework for Quantum Machine Learning},
	url = {http://arxiv.org/abs/2003.02989},
	shorttitle = {{TensorFlow} Quantum},
	abstract = {We introduce {TensorFlow} Quantum ({TFQ}), an open source library for the rapid prototyping of hybrid quantum-classical models for classical or quantum data. This framework offers high-level abstractions for the design and training of both discriminative and generative quantum models under {TensorFlow} and supports high-performance quantum circuit simulators. We provide an overview of the software architecture and building blocks through several examples and review the theory of hybrid quantum-classical neural networks. We illustrate {TFQ} functionalities via several basic applications including supervised learning for quantum classification, quantum control, and quantum approximate optimization. Moreover, we demonstrate how one can apply {TFQ} to tackle advanced quantum learning tasks including meta-learning, Hamiltonian learning, and sampling thermal states. We hope this framework provides the necessary tools for the quantum computing and machine learning research communities to explore models of both natural and artificial quantum systems, and ultimately discover new quantum algorithms which could potentially yield a quantum advantage.},
	journaltitle = {{arXiv}:2003.02989 [cond-mat, physics:quant-ph]},
	author = {Broughton, Michael and Verdon, Guillaume and {McCourt}, Trevor and Martinez, Antonio J. and Yoo, Jae Hyeon and Isakov, Sergei V. and Massey, Philip and Niu, Murphy Yuezhen and Halavati, Ramin and Peters, Evan and Leib, Martin and Skolik, Andrea and Streif, Michael and Von Dollen, David and {McClean}, Jarrod R. and Boixo, Sergio and Bacon, Dave and Ho, Alan K. and Neven, Hartmut and Mohseni, Masoud},
	urldate = {2021-08-16},
	date = {2020-03-05},
	eprinttype = {arxiv},
	eprint = {2003.02989},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Condensed Matter - Disordered Systems and Neural Networks, Quantum Physics},
}

@article{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.},
	journaltitle = {{arXiv}:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2020-06-01},
	date = {2020-05-28},
	eprinttype = {arxiv},
	eprint = {2005.14165},
	keywords = {Computer Science - Computation and Language},
}

@misc{brunner_entity_2020,
	title = {Entity Matching with Transformer Architectures - A Step Forward in Data Integration},
	url = {https://openproceedings.org/2020/conf/edbt/paper_205.pdf},
	doi = {10.5441/002/EDBT.2020.58},
	abstract = {Transformer architectures have proven to be very effective and provide state-of-the-art results in many natural language tasks. The attention-based architecture in combination with pre-training on large amounts of text lead to the recent breakthrough and a variety of slightly different implementations. In this paper we analyze how well four of the most recent attention-based transformer architectures ({BERT}[6], {XLNet}[33], {RoBERTa}[17] and {DistilBERT} [23]) perform on the task of entity matching - a crucial part of data integration. Entity matching ({EM}) is the task of finding data instances that refer to the same real-world entity. It is a challenging task if the data instances consist of long textual data or if the data instances are "dirty" due to misplaced values.},
	version = {1},
	publisher = {{OpenProceedings}.org},
	author = {Brunner, Ursin and Stockinger, Kurt},
	urldate = {2022-01-24},
	date = {2020},
	langid = {english},
	keywords = {Database Technology, \_tablet},
}

@article{brunner_identifiability_2020,
	title = {On Identifiability in Transformers},
	url = {http://arxiv.org/abs/1908.04211},
	abstract = {In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models.},
	journaltitle = {{arXiv}:1908.04211 [cs]},
	author = {Brunner, Gino and Liu, Yang and Pascual, Damián and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger},
	urldate = {2021-03-15},
	date = {2020-02-07},
	eprinttype = {arxiv},
	eprint = {1908.04211},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7, I.2.7, I.7.0, I.7.0},
}

@inproceedings{bucilua_model_2006,
	location = {New York, {NY}, {USA}},
	title = {Model compression},
	isbn = {978-1-59593-339-3},
	url = {https://doi.org/10.1145/1150402.1150464},
	doi = {10.1145/1150402.1150464},
	series = {{KDD} '06},
	abstract = {Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. {PDAs}), and where computational power is limited (e.g. hea-ring aids). We present a method for "compressing" large, complex ensembles into smaller, faster models, usually without significant loss in performance.},
	pages = {535--541},
	booktitle = {Proceedings of the 12th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Buciluǎ, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
	urldate = {2020-10-23},
	date = {2006-08-20},
	keywords = {model compression, supervised learning},
}

@article{cappelletti_polyadic_2020,
	title = {Polyadic Quantum Classifier},
	url = {http://arxiv.org/abs/2007.14044},
	abstract = {We introduce here a supervised quantum machine learning algorithm for multi-class classification on {NISQ} architectures. A parametric quantum circuit is trained to output a specific bit string corresponding to the class of the input datapoint. We train and test it on an {IBMq} 5-qubit quantum computer and the algorithm shows good accuracy –compared to a classical machine learning model– for ternary classification of the Iris dataset and an extension of the {XOR} problem. Furthermore, we evaluate with simulations how the algorithm fares for a binary and a quaternary classification on resp. a known binary dataset and a synthetic dataset.},
	journaltitle = {{arXiv}:2007.14044 [quant-ph]},
	author = {Cappelletti, William and Erbanni, Rebecca and Keller, Joaquín},
	urldate = {2021-02-12},
	date = {2020-07},
	keywords = {Quantum Physics},
}

@thesis{caruana_multitask_1997,
	title = {Multitask Learning},
	url = {http://reports-archive.adm.cs.cmu.edu/anon/1997/CMU-CS-97-203.pdf},
	institution = {School of Computer Science Carnegie Mellon University Pittsburgh, {PA} 15213},
	type = {phdthesis},
	author = {Caruana, Rich},
	date = {1997-09-23},
	langid = {english},
}

@article{cheng_long_2016,
	title = {Long Short-Term Memory-Networks for Machine Reading},
	url = {http://arxiv.org/abs/1601.06733},
	abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
	journaltitle = {{arXiv}:1601.06733 [cs]},
	author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
	urldate = {2020-03-30},
	date = {2016-09-20},
	eprinttype = {arxiv},
	eprint = {1601.06733},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@article{child_generating_2019,
	title = {Generating Long Sequences with Sparse Transformers},
	url = {http://arxiv.org/abs/1904.10509},
	abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n {\textbackslash}sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, {CIFAR}-10, and {ImageNet}-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
	journaltitle = {{arXiv}:1904.10509 [cs, stat]},
	author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
	urldate = {2020-07-13},
	date = {2019-04-23},
	eprinttype = {arxiv},
	eprint = {1904.10509},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{clark_whatever_2013,
	title = {Whatever next? Predictive brains, situated agents, and the future of cognitive science},
	volume = {36},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X12000477/type/journal_article},
	doi = {10.1017/S0140525X12000477},
	shorttitle = {Whatever next?},
	abstract = {Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this “hierarchical prediction machine” approach, concluding that it offers the best clue yet to the shape of a uniﬁed science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.},
	pages = {181--204},
	number = {3},
	journaltitle = {Behavioral and Brain Sciences},
	shortjournal = {Behav Brain Sci},
	author = {Clark, Andy},
	urldate = {2023-09-23},
	date = {2013-06},
	langid = {english},
}

@inproceedings{clark_electra_2019,
	title = {{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators},
	url = {https://openreview.net/forum?id=r1xMH1BtvB},
	shorttitle = {{ELECTRA}},
	abstract = {Masked language modeling ({MLM}) pre-training methods such as {BERT} corrupt the input by replacing some tokens with [{MASK}] and then train a model to reconstruct the original tokens. While they produce...},
	eventtitle = {International Conference on Learning Representations},
	author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
	urldate = {2020-03-30},
	date = {2019-09-25},
}

@article{clark_what_2019,
	title = {What Does {BERT} Look At? An Analysis of {BERT}'s Attention},
	url = {http://arxiv.org/abs/1906.04341},
	shorttitle = {What Does {BERT} Look At?},
	abstract = {Large pre-trained neural networks such as {BERT} have had great recent success in {NLP}, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to {BERT}. {BERT}'s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in {BERT}'s attention.},
	journaltitle = {{arXiv}:1906.04341 [cs]},
	author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
	urldate = {2021-03-15},
	date = {2019-06-10},
	eprinttype = {arxiv},
	eprint = {1906.04341},
	keywords = {Computer Science - Computation and Language},
}

@article{dai_transformer-xl:_2019,
	title = {Transformer-{XL}: Attentive Language Models Beyond a Fixed-Length Context},
	url = {http://arxiv.org/abs/1901.02860},
	shorttitle = {Transformer-{XL}},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-{XL} that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-{XL} learns dependency that is 80\% longer than {RNNs} and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on {WikiText}-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on {WikiText}-103, Transformer-{XL} manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and {PyTorch}.},
	journaltitle = {{arXiv}:1901.02860 [cs, stat]},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	urldate = {2019-08-28},
	date = {2019-01-09},
	eprinttype = {arxiv},
	eprint = {1901.02860},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, {NLP}, Statistics - Machine Learning, {XL}, language model, transformer},
}

@article{dathathri_plug_2020,
	title = {Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
	url = {http://arxiv.org/abs/1912.02164},
	shorttitle = {Plug and Play Language Models},
	abstract = {Large transformer-based language models ({LMs}) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model ({PPLM}) for controllable language generation, which combines a pretrained {LM} with one or more simple attribute classifiers that guide text generation without any further training of the {LM}. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the {LM}. Sampling entails a forward and backward pass in which gradients from the attribute model push the {LM}'s hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. {PPLMs} are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.},
	journaltitle = {{arXiv}:1912.02164 [cs]},
	author = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
	urldate = {2021-05-12},
	date = {2020-03-03},
	eprinttype = {arxiv},
	eprint = {1912.02164},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{de_lange_continual_2020,
	title = {A continual learning survey: Defying forgetting in classification tasks},
	url = {http://arxiv.org/abs/1909.08383},
	shorttitle = {A continual learning survey},
	abstract = {Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern 1) a taxonomy and extensive overview of the state-of-the-art, 2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner, 3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods and 4 baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced {iNaturalist} and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.},
	journaltitle = {{arXiv}:1909.08383 [cs, stat]},
	author = {De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ales and Slabaugh, Gregory and Tuytelaars, Tinne},
	urldate = {2020-11-07},
	date = {2020-05},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@article{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	journaltitle = {{arXiv}:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2021-05-20},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@article{doersch_tutorial_2016,
	title = {Tutorial on Variational Autoencoders},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders ({VAEs}) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. {VAEs} are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. {VAEs} have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, {CIFAR} images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind {VAEs}, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	journaltitle = {{arXiv}:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	urldate = {2020-11-03},
	date = {2016-08},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{domingos_master_2015,
	edition = {1st edition},
	title = {The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World},
	shorttitle = {The Master Algorithm},
	abstract = {A thought-provoking and wide-ranging exploration of machine learning and the race to build computer intelligences as flexible as our {ownIn} the world's top research labs and universities, the race is on to invent the ultimate learning algorithm: one capable of discovering any knowledge from data, and doing anything we want, before we even ask. In The Master Algorithm, Pedro Domingos lifts the veil to give us a peek inside the learning machines that power Google, Amazon, and your smartphone. He assembles a blueprint for the future universal learner--the Master Algorithm--and discusses what it will mean for business, science, and society. If data-ism is today's philosophy, this book is its bible.},
	pagetotal = {354},
	publisher = {Basic Books},
	author = {Domingos, Pedro},
	date = {2015-09-22},
}

@article{fei-fei_one-shot_2006,
	title = {One-shot learning of object categories},
	volume = {28},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2006.79},
	abstract = {Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood ({ML}) and maximum a posteriori ({MAP}) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.},
	pages = {594--611},
	number = {4},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Fei-Fei, Li and Fergus, R. and Perona, P.},
	date = {2006-04},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Automotive materials, Bayesian methods, Image databases, Layout, Management training, Probability density function, Recognition, Rough surfaces, Surface roughness, Taxonomy, Testing, few images, learning, object categories, priors., unsupervised, variational inference},
}

@article{fink_object_2004,
	title = {Object Classification from a Single Example Utilizing Class Relevance Metrics},
	volume = {17},
	url = {https://proceedings.neurips.cc/paper/2004/hash/ef1e491a766ce3127556063d49bc2f98-Abstract.html},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Fink, Michael},
	urldate = {2021-06-04},
	date = {2004},
	langid = {english},
}

@article{garcez_neural-symbolic_2019,
	title = {Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning},
	url = {http://arxiv.org/abs/1905.06088},
	shorttitle = {Neural-Symbolic Computing},
	abstract = {Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of {AI} have been raised by influential thinkers. In spite of the recent impact of {AI}, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable {AI} systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable {AI} systems.},
	journaltitle = {{arXiv}:1905.06088 [cs]},
	author = {Garcez, Artur d'Avila and Gori, Marco and Lamb, Luis C. and Serafini, Luciano and Spranger, Michael and Tran, Son N.},
	urldate = {2021-04-15},
	date = {2019-05-15},
	eprinttype = {arxiv},
	eprint = {1905.06088},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{gasparetto_survey_2022,
	title = {A Survey on Text Classification Algorithms: From Text to Predictions},
	volume = {13},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/13/2/83},
	doi = {10.3390/info13020083},
	shorttitle = {A Survey on Text Classification Algorithms},
	abstract = {In recent years, the exponential growth of digital documents has been met by rapid progress in text classification techniques. Newly proposed machine learning algorithms leverage the latest advancements in deep learning methods, allowing for the automatic extraction of expressive features. The swift development of these methods has led to a plethora of strategies to encode natural language into machine-interpretable data. The latest language modelling algorithms are used in conjunction with ad hoc preprocessing procedures, of which the description is often omitted in favour of a more detailed explanation of the classification step. This paper offers a concise review of recent text classification models, with emphasis on the flow of data, from raw text to output labels. We highlight the differences between earlier methods and more recent, deep learning-based methods in both their functioning and in how they transform input data. To give a better perspective on the text classification landscape, we provide an overview of datasets for the English language, as well as supplying instructions for the synthesis of two new multilabel datasets, which we found to be particularly scarce in this setting. Finally, we provide an outline of new experimental results and discuss the open research challenges posed by deep learning-based language models.},
	pages = {83},
	number = {2},
	journaltitle = {Information},
	author = {Gasparetto, Andrea and Marcuzzo, Matteo and Zangari, Alessandro and Albarelli, Andrea},
	urldate = {2022-06-04},
	date = {2022-02},
	langid = {english},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {\_tablet, deep learning, multilabel corpora, news classification, shallow learning, text classification, tokenisation, topic labelling, transformer},
}

@article{geiger_scaling_2020,
	title = {Scaling description of generalization with number of parameters in deep learning},
	volume = {2020},
	issn = {1742-5468},
	url = {http://arxiv.org/abs/1901.01608},
	doi = {10.1088/1742-5468/ab633c},
	abstract = {Supervised deep learning involves the training of neural networks with a large number \$N\$ of parameters. For large enough \$N\$, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as \$N\$ grows past a certain threshold \$N{\textasciicircum}\{*\}\$. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with \$N\$. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations \${\textbackslash}{\textbar}f\_\{N\}-{\textbackslash}bar\{f\}\_\{N\}{\textbackslash}{\textbar}{\textbackslash}sim N{\textasciicircum}\{-1/4\}\$ of the neural net output function \$f\_\{N\}\$ around its expectation \${\textbackslash}bar\{f\}\_\{N\}\$. These affect the generalization error \${\textbackslash}epsilon\_\{N\}\$ for classification: under natural assumptions, it decays to a plateau value \${\textbackslash}epsilon\_\{{\textbackslash}infty\}\$ in a power-law fashion \${\textbackslash}sim N{\textasciicircum}\{-1/2\}\$. This description breaks down at a so-called jamming transition \$N=N{\textasciicircum}\{*\}\$. At this threshold, we argue that \${\textbackslash}{\textbar}f\_\{N\}{\textbackslash}{\textbar}\$ diverges. This result leads to a plausible explanation for the cusp in test error known to occur at \$N{\textasciicircum}\{*\}\$. Our results are confirmed by extensive empirical observations on the {MNIST} and {CIFAR} image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond \$N{\textasciicircum}\{*\}\$, and averaging their outputs.},
	pages = {023401},
	number = {2},
	journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
	shortjournal = {J. Stat. Mech.},
	author = {Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d'Ascoli, Stéphane and Biroli, Giulio and Hongler, Clément and Wyart, Matthieu},
	urldate = {2020-05-11},
	date = {2020-02-05},
	eprinttype = {arxiv},
	eprint = {1901.01608},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
}

@article{goldberg_word2vec_2014,
	title = {word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method},
	url = {http://arxiv.org/abs/1402.3722},
	shorttitle = {word2vec Explained},
	abstract = {The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in "Distributed Representations of Words and Phrases and their Compositionality" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.},
	journaltitle = {{arXiv}:1402.3722 [cs, stat]},
	author = {Goldberg, Yoav and Levy, Omer},
	urldate = {2021-04-15},
	date = {2014-02-15},
	eprinttype = {arxiv},
	eprint = {1402.3722},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gong_hierarchical_2020,
	title = {Hierarchical Graph Transformer Based Deep Learning Model for Large-Scale Multi-Label Text Classification},
	volume = {{PP}},
	doi = {10.1109/ACCESS.2020.2972751},
	abstract = {Traditional methods of multi-label text classification, particularly deep learning, have achieved remarkable results. However, most of these methods use word2vec technology to represent sequential text information, while ignoring the logic and internal hierarchy of the text itself. Although these approaches can learn the hypothetical hierarchy and logic of the text, it is unexplained. In addition, the traditional approach treats labels as independent individuals and ignores the relationships between them, which not only does not reflect reality but also causes significant loss of semantic information. In this paper, we propose a novel Hierarchical Graph Transformer based deep learning model for large-scale multi-label text classification. We first model the text into a graph structure that can embody the different semantics of the text and the connections between them. We then use a multi-layer transformer structure with a multi-head attention mechanism at the word, sentence, and graph levels to fully capture the features of the text and observe the importance of the separate parts. Finally, we use the hierarchical relationship of the labels to generate the representation of the labels, and design a weighted loss function based on the semantic distances of the labels. Extensive experiments conducted on three benchmark datasets demonstrated that the proposed model can realistically capture the hierarchy and logic of text and improve performance compared with the state-of-the-art methods.},
	pages = {1--1},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Gong, Jibing and Liu, Mingsheng and Ma, Hongyuan and Teng, Zhiyong and Teng, Qi and Zhang, Hekai and Du, Linfeng and Chen, Shuai and Bhuiyan, Md and Li, Jianhua},
	date = {2020-02-10},
}

@article{goodfellow_generative_2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	journaltitle = {{arXiv}:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2020-11-04},
	date = {2014-06},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{goodfellow_deep_2016,
	location = {Cambridge, Massachusetts},
	title = {Deep Learning},
	isbn = {978-0-262-03561-3},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of {OpenAI}; cofounder and {CEO} of Tesla and {SpaceXDeep} learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	pagetotal = {800},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016-11-18},
}

@inproceedings{gupta_effective_2020,
	location = {Barcelona, Spain (Online)},
	title = {Effective Few-Shot Classification with Transfer Learning},
	url = {https://www.aclweb.org/anthology/2020.coling-main.92},
	doi = {10.18653/v1/2020.coling-main.92},
	abstract = {Few-shot learning addresses the the problem of learning based on a small amount of training data. Although more well-studied in the domain of computer vision, recent work has adapted the Amazon Review Sentiment Classification ({ARSC}) text dataset for use in the few-shot setting. In this work, we use the {ARSC} dataset to study a simple application of transfer learning approaches to few-shot classification. We train a single binary classifier to learn all few-shot classes jointly by prefixing class identifiers to the input text. Given the text and class, the model then makes a binary prediction for that text/class pair. Our results show that this simple approach can outperform most published results on this dataset. Surprisingly, we also show that including domain information as part of the task definition only leads to a modest improvement in model accuracy, and zero-shot classification, without further fine-tuning on few-shot domains, performs equivalently to few-shot classification. These results suggest that the classes in the {ARSC} few-shot task, which are defined by the intersection of domain and rating, are actually very similar to each other, and that a more suitable dataset is needed for the study of few-shot text classification.},
	eventtitle = {{COLING} 2020},
	pages = {1061--1066},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	publisher = {International Committee on Computational Linguistics},
	author = {Gupta, Aakriti and Thadani, Kapil and O'Hare, Neil},
	urldate = {2021-04-01},
	date = {2020-12},
}

@article{hinton_distilling_2015,
	title = {Distilling the Knowledge in a Neural Network},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on {MNIST} and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	journaltitle = {{arXiv}:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	urldate = {2020-10-23},
	date = {2015-03},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{huszar_quadratic_2017,
	title = {On Quadratic Penalties in Elastic Weight Consolidation},
	url = {http://arxiv.org/abs/1712.03847},
	abstract = {Elastic weight consolidation ({EWC}, Kirkpatrick et al, 2017) is a novel algorithm designed to safeguard against catastrophic forgetting in neural networks. {EWC} can be seen as an approximation to Laplace propagation (Eskin et al, 2004), and this view is consistent with the motivation given by Kirkpatrick et al (2017). In this note, I present an extended derivation that covers the case when there are more than two tasks. I show that the quadratic penalties in {EWC} are inconsistent with this derivation and might lead to double-counting data from earlier tasks.},
	journaltitle = {{arXiv}:1712.03847 [cs, stat]},
	author = {Huszár, Ferenc},
	urldate = {2020-11-07},
	date = {2017-12},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ioffe_batch_2015,
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	url = {http://arxiv.org/abs/1502.03167},
	shorttitle = {Batch Normalization},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on {ImageNet} classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	journaltitle = {{arXiv}:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	urldate = {2020-03-09},
	date = {2015-03-02},
	eprinttype = {arxiv},
	eprint = {1502.03167},
	keywords = {Computer Science - Machine Learning},
}

@article{jaderberg_quantum_2021,
	title = {Quantum Self-Supervised Learning},
	url = {http://arxiv.org/abs/2103.14653},
	abstract = {The popularisation of neural networks has seen incredible advances in pattern recognition, driven by the supervised learning of human annotations. However, this approach is unsustainable in relation to the dramatically increasing size of real-world datasets. This has led to a resurgence in self-supervised learning, a paradigm whereby the model generates its own supervisory signal from the data. Here we propose a hybrid quantum-classical neural network architecture for contrastive self-supervised learning and test its effectiveness in proof-of-principle experiments. Interestingly, we observe a numerical advantage for the learning of visual representations using small-scale quantum neural networks over equivalently structured classical networks, even when the quantum circuits are sampled with only 100 shots. Furthermore, we apply our best quantum model to classify unseen images on the ibmq\_paris quantum computer and find that current noisy devices can already achieve equal accuracy to the equivalent classical model on downstream tasks.},
	journaltitle = {{arXiv}:2103.14653 [quant-ph]},
	author = {Jaderberg, Ben and Anderson, Lewis W. and Xie, Weidi and Albanie, Samuel and Kiffner, Martin and Jaksch, Dieter},
	urldate = {2021-06-11},
	date = {2021-03-26},
	eprinttype = {arxiv},
	eprint = {2103.14653},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantum Physics},
}

@article{jaegle_perceiver_2021,
	title = {Perceiver: General Perception with Iterative Attention},
	url = {http://arxiv.org/abs/2103.03206},
	shorttitle = {Perceiver},
	abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like {ConvNets}. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to {ResNet}-50 and {ViT} on {ImageNet} without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in {AudioSet}.},
	journaltitle = {{arXiv}:2103.03206 [cs, eess]},
	author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
	urldate = {2022-01-21},
	date = {2021-06-22},
	eprinttype = {arxiv},
	eprint = {2103.03206},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{kaplan_scaling_2020,
	title = {Scaling Laws for Neural Language Models},
	url = {http://arxiv.org/abs/2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	journaltitle = {{arXiv}:2001.08361 [cs, stat]},
	author = {Kaplan, Jared and {McCandlish}, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	urldate = {2022-04-19},
	date = {2020-01-22},
	eprinttype = {arxiv},
	eprint = {2001.08361},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{karpathy_large-scale_nodate,
	title = {Large-scale Video Classiﬁcation with Convolutional Neural Networks},
	abstract = {Convolutional Neural Networks ({CNNs}) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of {CNNs} on largescale video classiﬁcation using a new dataset of 1 million {YouTube} videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a {CNN} in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display signiﬁcant performance improvements compared to strong feature-based baselines (55.3\% to 63.9\%), but only a surprisingly modest improvement compared to single-frame models (59.3\% to 60.9\%). We further study the generalization performance of our best model by retraining the top layers on the {UCF}101 Action Recognition dataset and observe signiﬁcant performance improvements compared to the {UCF}-101 baseline model (63.3\% up from 43.9\%).},
	pages = {8},
	author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
	langid = {english},
}

@article{karpathy_visualizing_2015,
	title = {Visualizing and Understanding Recurrent Networks},
	url = {http://arxiv.org/abs/1506.02078},
	abstract = {Recurrent Neural Networks ({RNNs}), and specifically a variant with Long Short-Term Memory ({LSTM}), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while {LSTMs} provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the {LSTM} improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
	journaltitle = {{arXiv}:1506.02078 [cs]},
	author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
	urldate = {2021-04-08},
	date = {2015-11-16},
	eprinttype = {arxiv},
	eprint = {1506.02078},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@online{keitakurita_building_2019,
	title = {Building the Transformer {XL} from Scratch},
	url = {https://mlexplained.com/2019/07/04/building-the-transformer-xl-from-scratch/},
	abstract = {With the release of {XLNet}, the Transformer {XL} is the new cool kid on the block. Although the Transformer {XL} is simple in concept, actually understanding the details is harder than might meet the ey…},
	titleaddon = {Machine Learning Explained},
	author = {keitakurita, Author},
	urldate = {2019-10-19},
	date = {2019-07-04},
	langid = {american},
}

@online{keitakurita_intuitive_2018,
	title = {An Intuitive Explanation of Why Batch Normalization Really Works (Normalization in Deep Learning Part 1)},
	url = {http://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/},
	abstract = {Batch normalization is one of the reasons why deep learning has made such outstanding progress in recent years. Batch normalization enables the use of higher learning rates, greatly accelerating th…},
	titleaddon = {Machine Learning Explained},
	author = {keitakurita, Author},
	urldate = {2020-03-06},
	date = {2018-01-10},
	langid = {american},
	note = {Library Catalog: mlexplained.com},
}

@inproceedings{kemker_fearnet_2018,
	title = {{FearNet}: Brain-Inspired Model for Incremental Learning},
	url = {https://openreview.net/forum?id=SJ1Xmf-Rb},
	shorttitle = {{FearNet}},
	abstract = {{FearNet} is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.},
	eventtitle = {International Conference on Learning Representations},
	author = {Kemker, Ronald and Kanan, Christopher},
	urldate = {2020-10-31},
	date = {2018-02-15},
	langid = {english},
}

@article{keskar_ctrl_nodate,
	title = {{CTRL}: A {CONDITIONAL} {TRANSFORMER} {LANGUAGE} {MODEL} {FOR} {CONTROLLABLE} {GENERATION}},
	abstract = {Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release {CTRL}, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-speciﬁc behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow {CTRL} to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of {CTRL} at https://github.com/salesforce/ctrl.},
	pages = {18},
	author = {Keskar, Nitish Shirish and {McCann}, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
	langid = {english},
	keywords = {Computer Science - Computation and Language},
}

@article{kirkpatrick_overcoming_2017,
	title = {Overcoming catastrophic forgetting in neural networks},
	url = {http://arxiv.org/abs/1612.00796},
	abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the {MNIST} hand written digit dataset and by learning several Atari 2600 games sequentially.},
	journaltitle = {{arXiv}:1612.00796 [cs, stat]},
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	urldate = {2020-11-07},
	date = {2017-01},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kirkpatrick_optimization_1983,
	title = {Optimization by Simulated Annealing},
	volume = {220},
	rights = {© 1983},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/220/4598/671},
	doi = {10.1126/science.220.4598.671},
	abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
	pages = {671--680},
	number = {4598},
	journaltitle = {Science},
	author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
	urldate = {2021-04-04},
	date = {1983-05-13},
	langid = {english},
	pmid = {17813860},
	note = {Publisher: American Association for the Advancement of Science
Section: Articles},
}

@article{kitaev_reformer_2020,
	title = {Reformer: The Efficient Transformer},
	url = {http://arxiv.org/abs/2001.04451},
	shorttitle = {Reformer},
	abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L{\textasciicircum}2\$) to O(\$L{\textbackslash}log L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
	journaltitle = {{arXiv}:2001.04451 [cs, stat]},
	author = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
	urldate = {2021-04-09},
	date = {2020-02-18},
	eprinttype = {arxiv},
	eprint = {2001.04451},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{klein_opennmt_2017,
	location = {Vancouver, Canada},
	title = {{OpenNMT}: Open-Source Toolkit for Neural Machine Translation},
	url = {http://aclweb.org/anthology/P17-4012},
	doi = {10.18653/v1/P17-4012},
	shorttitle = {{OpenNMT}},
	eventtitle = {Proceedings of {ACL} 2017, System Demonstrations},
	pages = {67--72},
	booktitle = {Proceedings of {ACL} 2017, System Demonstrations},
	publisher = {Association for Computational Linguistics},
	author = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander},
	urldate = {2020-03-06},
	date = {2017},
	langid = {english},
}

@article{kraska_case_2018,
	title = {The Case for Learned Index Structures},
	url = {http://arxiv.org/abs/1712.01208},
	abstract = {Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a {BitMap}-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. The key idea is that a model can learn the sort order or structure of lookup keys and use this signal to effectively predict the position or existence of records. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show, that by using neural nets we are able to outperform cache-optimized B-Trees by up to 70\% in speed while saving an order-of-magnitude in memory over several real-world data sets. More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work just provides a glimpse of what might be possible.},
	journaltitle = {{arXiv}:1712.01208 [cs]},
	author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
	urldate = {2021-05-21},
	date = {2018-04-30},
	eprinttype = {arxiv},
	eprint = {1712.01208},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases, Computer Science - Neural and Evolutionary Computing},
}

@misc{lake_building_2016,
	title = {Building Machines That Learn and Think Like People},
	url = {http://arxiv.org/abs/1604.00289},
	doi = {10.48550/arXiv.1604.00289},
	abstract = {Recent progress in artificial intelligence ({AI}) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	number = {{arXiv}:1604.00289},
	publisher = {{arXiv}},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	urldate = {2023-10-29},
	date = {2016-11-02},
	eprinttype = {arxiv},
	eprint = {1604.00289 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{lample_cross-lingual_2019,
	title = {Cross-lingual Language Model Pretraining},
	url = {http://arxiv.org/abs/1901.07291},
	abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models ({XLMs}): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On {XNLI}, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 {BLEU} on {WMT}'16 German-English, improving the previous state of the art by more than 9 {BLEU}. On supervised machine translation, we obtain a new state of the art of 38.5 {BLEU} on {WMT}'16 Romanian-English, outperforming the previous best approach by more than 4 {BLEU}. Our code and pretrained models will be made publicly available.},
	journaltitle = {{arXiv}:1901.07291 [cs]},
	author = {Lample, Guillaume and Conneau, Alexis},
	urldate = {2020-10-15},
	date = {2019-01},
	keywords = {Computer Science - Computation and Language},
}

@article{lecun_path_nodate,
	title = {A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27},
	abstract = {How could machines learn as eﬃciently as humans and animals? How could machines learn to reason and plan? How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons? This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as conﬁgurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.},
	pages = {62},
	author = {{LeCun}, Yann},
	langid = {english},
	keywords = {\_tablet},
}

@article{lee-thorp_fnet_2021,
	title = {{FNet}: Mixing Tokens with Fourier Transforms},
	url = {http://arxiv.org/abs/2105.03824},
	shorttitle = {{FNet}},
	abstract = {We show that Transformer encoder architectures can be massively sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that "mix" input tokens. These linear transformations, along with simple nonlinearities in feed-forward layers, are sufficient to model semantic relationships in several text classification tasks. Perhaps most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92\% of the accuracy of {BERT} on the {GLUE} benchmark, but pre-trains and runs up to seven times faster on {GPUs} and twice as fast on {TPUs}. The resulting model, which we name {FNet}, scales very efficiently to long inputs, matching the accuracy of the most accurate "efficient" Transformers on the Long Range Arena benchmark, but training and running faster across all sequence lengths on {GPUs} and relatively shorter sequence lengths on {TPUs}. Finally, {FNet} has a light memory footprint and is particularly efficient at smaller model sizes: for a fixed speed and accuracy budget, small {FNet} models outperform Transformer counterparts.},
	journaltitle = {{arXiv}:2105.03824 [cs]},
	author = {Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
	urldate = {2021-06-02},
	date = {2021-05-08},
	eprinttype = {arxiv},
	eprint = {2105.03824},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{li_learning_2017,
	title = {Learning without Forgetting},
	url = {http://arxiv.org/abs/1606.09282},
	abstract = {When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network ({CNN}), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
	journaltitle = {{arXiv}:1606.09282 [cs, stat]},
	author = {Li, Zhizhong and Hoiem, Derek},
	urldate = {2020-11-09},
	date = {2017-02},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_deep_2020,
	title = {Deep Entity Matching with Pre-Trained Language Models},
	volume = {14},
	issn = {2150-8097},
	url = {http://arxiv.org/abs/2004.00584},
	doi = {10.14778/3421424.3421431},
	abstract = {We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast {EM} as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straightforward application of language models such as {BERT}, {DistilBERT}, or {RoBERTa} pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art ({SOTA}), by up to 29\% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto's matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for {EM}. Finally, Ditto adapts a {SOTA} technique on data augmentation for text to {EM} to augment the training data with (difficult) examples. This way, Ditto is forced to learn "harder" to improve the model's matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8\%. Perhaps more surprisingly, we establish that Ditto can achieve the previous {SOTA} results with at most half the number of labeled data. Finally, we demonstrate Ditto's effectiveness on a real-world large-scale {EM} task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5\%.},
	pages = {50--60},
	number = {1},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Li, Yuliang and Li, Jinfeng and Suhara, Yoshihiko and Doan, {AnHai} and Tan, Wang-Chiew},
	urldate = {2022-01-24},
	date = {2020-09},
	eprinttype = {arxiv},
	eprint = {2004.00584},
	keywords = {Computer Science - Computation and Language, Computer Science - Databases},
}

@report{li_deep_2020-1,
	title = {Deep Entity Matching with Pre-Trained Language Models},
	url = {http://arxiv.org/abs/2004.00584},
	abstract = {We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast {EM} as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straightforward application of language models such as {BERT}, {DistilBERT}, or {RoBERTa} pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art ({SOTA}), by up to 29\% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto's matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for {EM}. Finally, Ditto adapts a {SOTA} technique on data augmentation for text to {EM} to augment the training data with (difficult) examples. This way, Ditto is forced to learn "harder" to improve the model's matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8\%. Perhaps more surprisingly, we establish that Ditto can achieve the previous {SOTA} results with at most half the number of labeled data. Finally, we demonstrate Ditto's effectiveness on a real-world large-scale {EM} task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5\%.},
	author = {Li, Yuliang and Li, Jinfeng and Suhara, Yoshihiko and Doan, {AnHai} and Tan, Wang-Chiew},
	urldate = {2022-06-04},
	date = {2020-09-02},
	doi = {10.14778/3421424.3421431},
	eprinttype = {arxiv},
	eprint = {2004.00584 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Databases},
}

@article{li_survey_2022,
	title = {A Survey on Text Classification: From Traditional to Deep Learning},
	volume = {13},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3495162},
	doi = {10.1145/3495162},
	shorttitle = {A Survey on Text Classification},
	abstract = {Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021, focusing on models from traditional models to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.},
	pages = {31:1--31:41},
	number = {2},
	journaltitle = {{ACM} Transactions on Intelligent Systems and Technology},
	shortjournal = {{ACM} Trans. Intell. Syst. Technol.},
	author = {Li, Qian and Peng, Hao and Li, Jianxin and Xia, Congying and Yang, Renyu and Sun, Lichao and Yu, Philip S. and He, Lifang},
	urldate = {2022-06-04},
	date = {2022-04-08},
	keywords = {Deep learning, \_tablet, challenges, evaluation metrics, text classification, traditional models},
}

@article{liu_roberta:_2019,
	title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
	url = {http://arxiv.org/abs/1907.11692},
	shorttitle = {{RoBERTa}},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	journaltitle = {{arXiv}:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	urldate = {2019-08-28},
	date = {2019-07-26},
	eprinttype = {arxiv},
	eprint = {1907.11692},
	keywords = {{BERT}, Computer Science - Computation and Language, {NLP}, language model},
}

@inproceedings{liu_multi-task_2019,
	location = {Florence, Italy},
	title = {Multi-Task Deep Neural Networks for Natural Language Understanding},
	url = {https://www.aclweb.org/anthology/P19-1441},
	doi = {10.18653/v1/P19-1441},
	abstract = {In this paper, we present a Multi-Task Deep Neural Network ({MT}-{DNN}) for learning representations across multiple natural language understanding ({NLU}) tasks. {MT}-{DNN} not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. {MT}-{DNN} extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as {BERT} (Devlin et al., 2018). {MT}-{DNN} obtains new state-of-the-art results on ten {NLU} tasks, including {SNLI}, {SciTail}, and eight out of nine {GLUE} tasks, pushing the {GLUE} benchmark to 82.7\% (2.2\% absolute improvement) as of February 25, 2019 on the latest {GLUE} test set. We also demonstrate using the {SNLI} and {SciTail} datasets that the representations learned by {MT}-{DNN} allow domain adaptation with substantially fewer in-domain labels than the pre-trained {BERT} representations. Our code and pre-trained models will be made publicly available.},
	eventtitle = {{ACL} 2019},
	pages = {4487--4496},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
	urldate = {2020-03-30},
	date = {2019-07},
}

@article{liu_seeing_nodate,
	title = {{SEEING} {IS} {BELIEVING}: {BRAIN}-{INSPIRED} {MODULAR} {TRAINING} {FOR} {MECHANISTIC} {INTERPRETABILITY}},
	abstract = {We introduce Brain-Inspired Modular Training ({BIMT}), a method for making neural networks more modular and interpretable. Inspired by brains, {BIMT} embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that {BIMT} discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.},
	author = {Liu, Ziming and Gan, Eric and Tegmark, Max},
	langid = {english},
}

@article{lloyd_quantum_2020,
	title = {Quantum embeddings for machine learning},
	url = {http://arxiv.org/abs/2001.03622},
	abstract = {Quantum classifiers are trainable quantum circuits used as machine learning models. The first part of the circuit implements a quantum feature map that encodes classical inputs into quantum states, embedding the data in a high-dimensional Hilbert space; the second part of the circuit executes a quantum measurement interpreted as the output of the model. Usually, the measurement is trained to distinguish quantum-embedded data. We propose to instead train the first part of the circuit---the embedding---with the objective of maximally separating data classes in Hilbert space, a strategy we call quantum metric learning. As a result, the measurement minimizing a linear classification loss is already known and depends on the metric used: for embeddings separating data using the l1 or trace distance, this is the Helstrom measurement, while for the l2 or Hilbert-Schmidt distance, it is a simple overlap measurement. This approach provides a powerful analytic framework for quantum machine learning and eliminates a major component in current models, freeing up more precious resources to best leverage the capabilities of near-term quantum information processors.},
	journaltitle = {{arXiv}:2001.03622 [quant-ph]},
	author = {Lloyd, Seth and Schuld, Maria and Ijaz, Aroosa and Izaac, Josh and Killoran, Nathan},
	urldate = {2021-06-12},
	date = {2020-02-10},
	eprinttype = {arxiv},
	eprint = {2001.03622},
	keywords = {Quantum Physics},
}

@article{lopez-paz_gradient_2017,
	title = {Gradient Episodic Memory for Continual Learning},
	url = {http://arxiv.org/abs/1706.08840},
	abstract = {One major obstacle towards {AI} is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory ({GEM}) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the {MNIST} and {CIFAR}-100 datasets demonstrate the strong performance of {GEM} when compared to the state-of-the-art.},
	journaltitle = {{arXiv}:1706.08840 [cs]},
	author = {Lopez-Paz, David and Ranzato, Marc'Aurelio},
	urldate = {2020-11-05},
	date = {2017-11},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{lundberg_unified_2017,
	title = {A Unified Approach to Interpreting Model Predictions},
	url = {http://arxiv.org/abs/1705.07874},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, {SHAP} ({SHapley} Additive {exPlanations}). {SHAP} assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	journaltitle = {{arXiv}:1705.07874 [cs, stat]},
	author = {Lundberg, Scott and Lee, Su-In},
	urldate = {2021-04-14},
	date = {2017-11-24},
	eprinttype = {arxiv},
	eprint = {1705.07874},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{luo_have_2021,
	title = {Have Attention Heads in {BERT} Learned Constituency Grammar?},
	url = {http://arxiv.org/abs/2102.07926},
	abstract = {With the success of pre-trained language models in recent years, more and more researchers focus on opening the "black box" of these models. Following this interest, we carry out a qualitative and quantitative analysis of constituency grammar in attention heads of {BERT} and {RoBERTa}. We employ the syntactic distance method to extract implicit constituency grammar from the attention weights of each head. Our results show that there exist heads that can induce some grammar types much better than baselines, suggesting that some heads act as a proxy for constituency grammar. We also analyze how attention heads' constituency grammar inducing ({CGI}) ability changes after fine-tuning with two kinds of tasks, including sentence meaning similarity ({SMS}) tasks and natural language inference ({NLI}) tasks. Our results suggest that {SMS} tasks decrease the average {CGI} ability of upper layers, while {NLI} tasks increase it. Lastly, we investigate the connections between {CGI} ability and natural language understanding ability on {QQP} and {MNLI} tasks.},
	journaltitle = {{arXiv}:2102.07926 [cs]},
	author = {Luo, Ziyang},
	urldate = {2021-03-16},
	date = {2021-02-15},
	eprinttype = {arxiv},
	eprint = {2102.07926},
	keywords = {Computer Science - Computation and Language},
}

@article{marcus_next_2020,
	title = {The Next Decade in {AI}: Four Steps Towards Robust Artificial Intelligence},
	url = {http://arxiv.org/abs/2002.06177},
	shorttitle = {The Next Decade in {AI}},
	abstract = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust {AI} than is currently possible.},
	journaltitle = {{arXiv}:2002.06177 [cs]},
	author = {Marcus, Gary},
	urldate = {2021-04-15},
	date = {2020-02-19},
	eprinttype = {arxiv},
	eprint = {2002.06177},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2, I.2.6},
}

@article{masse_alleviating_2018,
	title = {Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/44/E10467},
	doi = {10.1073/pnas.1803839115},
	abstract = {Humans and most animals can learn new tasks without forgetting old ones. However, training artificial neural networks ({ANNs}) on new tasks typically causes them to forget previously learned tasks. This phenomenon is the result of “catastrophic forgetting,” in which training an {ANN} disrupts connection weights that were important for solving previous tasks, degrading task performance. Several recent studies have proposed methods to stabilize connection weights of {ANNs} that are deemed most important for solving a task, which helps alleviate catastrophic forgetting. Here, drawing inspiration from algorithms that are believed to be implemented in vivo, we propose a complementary method: adding a context-dependent gating signal, such that only sparse, mostly nonoverlapping patterns of units are active for any one task. This method is easy to implement, requires little computational overhead, and allows {ANNs} to maintain high performance across large numbers of sequentially presented tasks, particularly when combined with weight stabilization. We show that this method works for both feedforward and recurrent network architectures, trained using either supervised or reinforcement-based learning. This suggests that using multiple, complementary methods, akin to what is believed to occur in the brain, can be a highly effective strategy to support continual learning.},
	pages = {E10467--E10475},
	number = {44},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Masse, Nicolas Y. and Grant, Gregory D. and Freedman, David J.},
	urldate = {2020-11-06},
	date = {2018-10-30},
	pmid = {30315147},
	keywords = {artificial intelligence, catastrophic forgetting, context-dependent gating, continual learning, synaptic stabilization},
}

@incollection{mccloskey_catastrophic_1989,
	title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
	volume = {24},
	url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
	shorttitle = {Catastrophic Interference in Connectionist Networks},
	abstract = {Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.},
	pages = {109--165},
	booktitle = {Psychology of Learning and Motivation},
	publisher = {Academic Press},
	author = {{McCloskey}, Michael and Cohen, Neal J.},
	editor = {Bower, Gordon H.},
	urldate = {2021-04-09},
	date = {1989-01-01},
	langid = {english},
	doi = {10.1016/S0079-7421(08)60536-8},
}

@book{metz_genius_2021,
	location = {New York},
	title = {Genius Makers: The Mavericks Who Brought {AI} to Google, Facebook, and the World},
	isbn = {978-1-5247-4267-6},
	shorttitle = {Genius Makers},
	abstract = {"This colorful page-turner puts artificial intelligence into a human perspective. Through the lives of Geoff Hinton and other major players, Metz explains this transformative technology and makes the quest thrilling."—Walter Isaacson, author of The Code {BreakerRecipient} of starred reviews in both Kirkus and Library {JournalTHE} {UNTOLD} {TECH} {STORY} {OF} {OUR} {TIME}   What does it mean to be smart? To be human? What do we really want from life and the intelligence we have, or might create?   With deep and exclusive reporting, across hundreds of interviews, New York Times Silicon Valley journalist Cade Metz brings you into the rooms where these questions are being answered. Where an extraordinarily powerful new artificial intelligence has been built into our biggest companies, our social discourse, and our daily lives, with few of us even noticing.     Long dismissed as a technology of the distant future, artificial intelligence was a project consigned to the fringes of the scientific community. Then two researchers changed everything. One was a sixty-four-year-old computer science professor who didn’t drive and didn’t fly because he could no longer sit down—but still made his way across North America for the moment that would define a new age of technology. The other was a thirty-six-year-old neuroscientist and chess prodigy who laid claim to being the greatest game player of all time before vowing to build a machine that could do anything the human brain could do.   They took two very different paths to that lofty goal, and they disagreed on how quickly it would arrive. But both were soon drawn into the heart of the tech industry. Their ideas drove a new kind of arms race, spanning Google, Microsoft, Facebook, and {OpenAI}, a new lab founded by Silicon Valley kingpin Elon Musk. But some believed that China would beat them all to the finish line.   Genius Makers dramatically presents the fierce conflict between national interests, shareholder value, the pursuit of scientific knowledge, and the very human concerns about privacy, security, bias, and prejudice. Like a great Victorian novel, this world of eccentric, brilliant, often unimaginably yet suddenly wealthy characters draws you into the most profound moral questions we can ask. And like a great mystery, it presents the story and facts that lead to a core, vital question:   How far will we let it go?},
	pagetotal = {384},
	author = {Metz, Cade},
	date = {2021-03-16},
}

@article{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	journaltitle = {{arXiv}:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2021-04-15},
	date = {2013-09-06},
	eprinttype = {arxiv},
	eprint = {1301.3781},
	keywords = {Computer Science - Computation and Language},
}

@article{mikolov_distributed_nodate,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	abstract = {The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
	pages = {9},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	langid = {english},
}

@article{miller_explanation_2018,
	title = {Explanation in Artificial Intelligence: Insights from the Social Sciences},
	url = {http://arxiv.org/abs/1706.07269},
	shorttitle = {Explanation in Artificial Intelligence},
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	journaltitle = {{arXiv}:1706.07269 [cs]},
	author = {Miller, Tim},
	urldate = {2021-04-07},
	date = {2018-08-14},
	eprinttype = {arxiv},
	eprint = {1706.07269},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{minaee_deep_2021,
	title = {Deep Learning Based Text Classification: A Comprehensive Review},
	url = {http://arxiv.org/abs/2004.03705},
	shorttitle = {Deep Learning Based Text Classification},
	abstract = {Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.},
	journaltitle = {{arXiv}:2004.03705 [cs, stat]},
	author = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
	urldate = {2021-03-04},
	date = {2021-01-04},
	eprinttype = {arxiv},
	eprint = {2004.03705},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, \_tablet},
}

@online{minsky_framework_1974,
	title = {A Framework for Representing Knowledge},
	url = {https://web.media.mit.edu/~minsky/papers/Frames/frames.html},
	author = {Minsky, Marvin},
	urldate = {2021-04-09},
	date = {1974},
}

@article{minsky_k-lines_1979,
	title = {K-Lines: A Theory of Memory},
	url = {https://dspace.mit.edu/handle/1721.1/5739},
	shorttitle = {K-Lines},
	abstract = {Most theories of memory suggest that when  we learn or memorize something, some  "representation" of that something is  constructed, stored and later retrieved. This  raises questions like: How is information  represented? How is it stored? How is it  retrieved? Then, how is it use? This paper  tries to deal with all these at once. When you  get an idea and want to "remember" it, you  create a "K-line" for it. When later activated, the  K-line induces a partial mental state  resembling the one that created it. A "partial  mental state" is a subset of those mental  agencies operating at one moment. This view  leads to many ideas about the development,  structure and physiology of Memory, and  about how to implement frame-like  representations in a distributed processor.},
	author = {Minsky, Marvin},
	urldate = {2021-04-09},
	date = {1979-06-01},
	langid = {american},
	note = {Accepted: 2004-10-01T20:33:25Z},
}

@book{minsky_perceptrons_1987,
	location = {Cambridge, Mass},
	title = {Perceptrons: An Introduction to Computational Geometry, Expanded Edition},
	isbn = {978-0-262-63111-2},
	shorttitle = {Perceptrons},
	abstract = {Perceptrons - the first systematic study of parallelism in computation - has remained a classical work on threshold automata networks for nearly two decades. It marked a historical turn in artificial intelligence, and it is required reading for anyone who wants to understand the connectionist counterrevolution that is going on today. Artificial-intelligence research, which for a time concentrated on the programming of ton Neumann computers, is swinging back to the idea that intelligence might emerge from the activity of networks of neuronlike entities. Minsky and Papert's book was the first example of a mathematical analysis carried far enough to show the exact limitations of a class of computing machines that could seriously be considered as models of the brain. Now the new developments in mathematical tools, the recent interest of physicists in the theory of disordered matter, the new insights into and psychological models of how the brain works, and the evolution of fast computers that can simulate networks of automata have given Perceptrons new importance.Witnessing the swing of the intellectual pendulum, Minsky and Papert have added a new chapter in which they discuss the current state of parallel computers, review developments since the appearance of the 1972 edition, and identify new research directions related to connectionism. They note a central theoretical challenge facing connectionism: the challenge to reach a deeper understanding of how "objects" or "agents" with individuality can emerge in a network. Progress in this area would link connectionism with what the authors have called "society theories of mind." Marvin L. Minsky is Donner Professor of Science in M.I.T.'s Electrical Engineering and Computer Science Department. Seymour A. Papert is Professor of Media Technology at M.I.T. .},
	pagetotal = {308},
	author = {Minsky, Marvin and Papert, Seymour A.},
	date = {1987},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	pages = {529--533},
	number = {7540},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	urldate = {2020-02-19},
	date = {2015-02},
	langid = {english},
}

@book{molnar_interpretable_nodate,
	title = {Interpretable Machine Learning},
	url = {https://christophm.github.io/interpretable-ml-book/},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	author = {Molnar, Christoph},
	urldate = {2021-04-14},
}

@inproceedings{mudgal_deep_2018,
	location = {Houston {TX} {USA}},
	title = {Deep Learning for Entity Matching: A Design Space Exploration},
	isbn = {978-1-4503-4703-7},
	url = {https://dl.acm.org/doi/10.1145/3183713.3196926},
	doi = {10.1145/3183713.3196926},
	shorttitle = {Deep Learning for Entity Matching},
	abstract = {Entity matching ({EM}) finds data instances that refer to the same real-world entity. In this paper we examine applying deep learning ({DL}) to {EM}, to understand {DL}’s benefits and limitations. We review many {DL} solutions that have been developed for related matching tasks in text processing (e.g., entity linking, textual entailment, etc.). We categorize these solutions and define a space of {DL} solutions for {EM}, as embodied by four solutions with varying representational power: {SIF}, {RNN}, Attention, and Hybrid. Next, we investigate the types of {EM} problems for which {DL} can be helpful. We consider three such problem types, which match structured data instances, textual instances, and dirty instances, respectively. We empirically compare the above four {DL} solutions with Magellan, a state-of-the-art learning-based {EM} solution. The results show that {DL} does not outperform current solutions on structured {EM}, but it can significantly outperform them on textual and dirty {EM}. For practitioners, this suggests that they should seriously consider using {DL} for textual and dirty {EM} problems. Finally, we analyze {DL}’s performance and discuss future research directions.},
	eventtitle = {{SIGMOD}/{PODS} '18: International Conference on Management of Data},
	pages = {19--34},
	booktitle = {Proceedings of the 2018 International Conference on Management of Data},
	publisher = {{ACM}},
	author = {Mudgal, Sidharth and Li, Han and Rekatsinas, Theodoros and Doan, {AnHai} and Park, Youngchoon and Krishnan, Ganesh and Deep, Rohit and Arcaute, Esteban and Raghavendra, Vijay},
	urldate = {2022-01-24},
	date = {2018-05-27},
	langid = {english},
}

@article{nakkiran_deep_2019,
	title = {Deep Double Descent: Where Bigger Models and More Data Hurt},
	url = {http://arxiv.org/abs/1912.02292},
	shorttitle = {Deep Double Descent},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	journaltitle = {{arXiv}:1912.02292 [cs, stat]},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	urldate = {2020-05-11},
	date = {2019-12-04},
	eprinttype = {arxiv},
	eprint = {1912.02292},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{pan_survey_2010,
	title = {A Survey on Transfer Learning},
	volume = {22},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5288526/},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classiﬁcation task in one domain of interest, but we only have sufﬁcient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classiﬁcation, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
	pages = {1345--1359},
	number = {10},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	urldate = {2020-10-15},
	date = {2010-10},
	langid = {english},
	keywords = {transfer learning},
}

@article{parisi_continual_2019,
	title = {Continual Lifelong Learning with Neural Networks: A Review},
	volume = {113},
	issn = {08936080},
	url = {http://arxiv.org/abs/1802.07569},
	doi = {10.1016/j.neunet.2019.01.012.},
	shorttitle = {Continual Lifelong Learning with Neural Networks},
	abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
	pages = {54--71},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
	urldate = {2020-09-22},
	date = {2019-05},
	eprinttype = {arxiv},
	eprint = {1802.07569},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@book{pearl_book_2020,
	title = {The Book of Why: The New Science of Cause and Effect},
	isbn = {978-1-5416-9896-3},
	shorttitle = {The Book of Why},
	abstract = {A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence "Correlation is not causation." This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality -- the study of cause and effect -- on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.},
	pagetotal = {432},
	author = {Pearl, Judea and Mackenzie, Dana},
	date = {2020},
}

@article{peng_hierarchical_2019,
	title = {Hierarchical Taxonomy-Aware and Attentional Graph Capsule {RCNNs} for Large-Scale Multi-Label Text Classification},
	url = {http://arxiv.org/abs/1906.04898},
	abstract = {{CNNs}, {RNNs}, {GCNs}, and {CapsNets} have shown significant insights in representation learning and are widely used in various text mining tasks such as large-scale multi-label text classification. However, most existing deep models for multi-label text classification consider either the non-consecutive and long-distance semantics or the sequential semantics, but how to consider them both coherently is less studied. In addition, most existing methods treat output labels as independent methods, but ignore the hierarchical relations among them, leading to useful semantic information loss. In this paper, we propose a novel hierarchical taxonomy-aware and attentional graph capsule recurrent {CNNs} framework for large-scale multi-label text classification. Specifically, we first propose to model each document as a word order preserved graph-of-words and normalize it as a corresponding words-matrix representation which preserves both the non-consecutive, long-distance and local sequential semantics. Then the words-matrix is input to the proposed attentional graph capsule recurrent {CNNs} for more effectively learning the semantic features. To leverage the hierarchical relations among the class labels, we propose a hierarchical taxonomy embedding method to learn their representations, and define a novel weighted margin loss by incorporating the label representation similarity. Extensive evaluations on three datasets show that our model significantly improves the performance of large-scale multi-label text classification by comparing with state-of-the-art approaches.},
	journaltitle = {{arXiv}:1906.04898 [cs, stat]},
	author = {Peng, Hao and Li, Jianxin and Gong, Qiran and Wang, Senzhang and He, Lifang and Li, Bo and Wang, Lihong and Yu, Philip S.},
	urldate = {2020-10-30},
	date = {2019-06},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model ({biLM}), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging {NLP} problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	journaltitle = {{arXiv}:1802.05365 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	urldate = {2021-04-15},
	date = {2018-03-22},
	eprinttype = {arxiv},
	eprint = {1802.05365},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{polyzotis_slice_2019,
	title = {Slice Finder: Automated Data Slicing for Model Validation},
	url = {https://arxiv.org/pdf/1807.06068.pdf},
	shorttitle = {Slice Finder},
	booktitle = {Proceedings of the {IEEE} Int' Conf. on Data Engineering ({ICDE}), 2019},
	author = {Polyzotis, Neoklis and Whang, Steven and Kraska, Tim Klas and Chung, Yeounoh},
	urldate = {2021-04-07},
	date = {2019},
}

@inproceedings{primpeli_profiling_2020,
	location = {Virtual Event Ireland},
	title = {Profiling Entity Matching Benchmark Tasks},
	isbn = {978-1-4503-6859-9},
	url = {https://dl.acm.org/doi/10.1145/3340531.3412781},
	doi = {10.1145/3340531.3412781},
	abstract = {Entity matching is a central task in data integration which has been researched for decades. Over this time, a wide range of benchmark tasks for evaluating entity matching methods has been developed. This resource paper systematically complements, profiles, and compares 21 entity matching benchmark tasks. In order to better understand the specific challenges associated with different tasks, we define a set of profiling dimensions which capture central aspects of the matching tasks. Using these dimensions, we create groups of benchmark tasks having similar characteristics. Afterwards, we assess the difficulty of the tasks in each group by computing baseline evaluation results using standard feature engineering together with two common classification methods. In order to enable the exact reproducibility of evaluation results, matching tasks need to contain exactly defined sets of matching and non-matching record pairs, as well as a fixed development and test split. As this is not the case for some widely-used benchmark tasks, we complement these tasks with fixed sets of non-matching pairs, as well as fixed splits, and provide the resulting development and test sets for public download. By profiling and complementing the benchmark tasks, we support researchers to select challenging as well as diverse tasks and to compare matching systems on clearly defined grounds.},
	eventtitle = {{CIKM} '20: The 29th {ACM} International Conference on Information and Knowledge Management},
	pages = {3101--3108},
	booktitle = {Proceedings of the 29th {ACM} International Conference on Information \& Knowledge Management},
	publisher = {{ACM}},
	author = {Primpeli, Anna and Bizer, Christian},
	urldate = {2022-03-18},
	date = {2020-10-19},
	langid = {english},
	keywords = {\_tablet},
}

@online{prukalpa_future_2022,
	title = {The Future of the Modern Data Stack in 2022},
	url = {https://towardsdatascience.com/the-future-of-the-modern-data-stack-in-2022-4f4c91bb778f},
	abstract = {Featuring the 6 big ideas you should know from 2021},
	titleaddon = {Medium},
	author = {Prukalpa},
	urldate = {2022-06-04},
	date = {2022-01-13},
	langid = {english},
}

@article{radford_language_nodate,
	title = {Language Models are Unsupervised Multitask Learners},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called {WebText}. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the {CoQA} dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, {GPT}-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts {WebText}. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	pages = {24},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	langid = {english},
}

@article{raffel_exploring_2020,
	title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing ({NLP}). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for {NLP} by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for {NLP}, we release our data set, pre-trained models, and code.},
	journaltitle = {{arXiv}:1910.10683 [cs, stat]},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	urldate = {2021-05-27},
	date = {2020-07-28},
	eprinttype = {arxiv},
	eprint = {1910.10683},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rajbhandari_deepspeed-moe_2022,
	title = {{DeepSpeed}-{MoE}: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation {AI} Scale},
	url = {http://arxiv.org/abs/2201.05596},
	shorttitle = {{DeepSpeed}-{MoE}},
	abstract = {As the training of giant dense models hits the boundary on the availability and capability of the hardware resources today, Mixture-of-Experts ({MoE}) models become one of the most promising model architectures due to their significant training cost reduction compared to a quality-equivalent dense model. Its training cost saving is demonstrated from encoder-decoder models (prior works) to a 5x saving for auto-aggressive language models (this work along with parallel explorations). However, due to the much larger model size and unique architecture, how to provide fast {MoE} model inference remains challenging and unsolved, limiting its practical usage. To tackle this, we present {DeepSpeed}-{MoE}, an end-to-end {MoE} training and inference solution as part of the {DeepSpeed} library, including novel {MoE} architecture designs and model compression techniques that reduce {MoE} model size by up to 3.7x, and a highly optimized inference system that provides 7.3x better latency and cost compared to existing {MoE} inference solutions. {DeepSpeed}-{MoE} offers an unprecedented scale and efficiency to serve massive {MoE} models with up to 4.5x faster and 9x cheaper inference compared to quality-equivalent dense models. We hope our innovations and systems help open a promising path to new directions in the large model landscape, a shift from dense to sparse {MoE} models, where training and deploying higher-quality models with fewer resources becomes more widely possible.},
	journaltitle = {{arXiv}:2201.05596 [cs]},
	author = {Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
	urldate = {2022-04-19},
	date = {2022-01-14},
	eprinttype = {arxiv},
	eprint = {2201.05596},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@article{rajpurkar_squad_2016,
	title = {{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
	url = {http://arxiv.org/abs/1606.05250},
	shorttitle = {{SQuAD}},
	abstract = {We present the Stanford Question Answering Dataset ({SQuAD}), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
	journaltitle = {{arXiv}:1606.05250 [cs]},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	urldate = {2021-05-21},
	date = {2016-10-10},
	eprinttype = {arxiv},
	eprint = {1606.05250},
	keywords = {Computer Science - Computation and Language},
}

@article{real_automl-zero_2020,
	title = {{AutoML}-Zero: Evolving Machine Learning Algorithms From Scratch},
	url = {http://arxiv.org/abs/2003.03384},
	shorttitle = {{AutoML}-Zero},
	abstract = {Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as {AutoML}, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that {AutoML} can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. {CIFAR}-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.},
	journaltitle = {{arXiv}:2003.03384 [cs, stat]},
	author = {Real, Esteban and Liang, Chen and So, David R. and Le, Quoc V.},
	urldate = {2020-04-20},
	date = {2020-03-06},
	eprinttype = {arxiv},
	eprint = {2003.03384},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.2, I.2.6, Statistics - Machine Learning},
}

@article{rebuffel_controlling_2021,
	title = {Controlling Hallucinations at Word Level in Data-to-Text Generation},
	url = {http://arxiv.org/abs/2102.02810},
	abstract = {Data-to-Text Generation ({DTG}) is a subfield of Natural Language Generation aiming at transcribing structured data in natural language descriptions. The field has been recently boosted by the use of neural-based generators which exhibit on one side great syntactic skills without the need of hand-crafted pipelines; on the other side, the quality of the generated text reflects the quality of the training data, which in realistic settings only offer imperfectly aligned structure-text pairs. Consequently, state-of-art neural models include misleading statements - usually called hallucinations - in their outputs. The control of this phenomenon is today a major challenge for {DTG}, and is the problem addressed in the paper. Previous work deal with this issue at the instance level: using an alignment score for each table-reference pair. In contrast, we propose a finer-grained approach, arguing that hallucinations should rather be treated at the word level. Specifically, we propose a Multi-Branch Decoder which is able to leverage word-level labels to learn the relevant parts of each training instance. These labels are obtained following a simple and efficient scoring procedure based on co-occurrence analysis and dependency parsing. Extensive evaluations, via automated metrics and human judgment on the standard {WikiBio} benchmark, show the accuracy of our alignment labels and the effectiveness of the proposed Multi-Branch Decoder. Our model is able to reduce and control hallucinations, while keeping fluency and coherence in generated texts. Further experiments on a degraded version of {ToTTo} show that our model could be successfully used on very noisy settings.},
	journaltitle = {{arXiv}:2102.02810 [cs]},
	author = {Rebuffel, Clément and Roberti, Marco and Soulier, Laure and Scoutheeten, Geoffrey and Cancelliere, Rossella and Gallinari, Patrick},
	urldate = {2021-05-13},
	date = {2021-02-04},
	eprinttype = {arxiv},
	eprint = {2102.02810},
	keywords = {68T50 (Primary), 68T07 (Secondary), 68T05, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.6, I.2.7},
}

@article{ribeiro_beyond_2020,
	title = {Beyond Accuracy: Behavioral Testing of {NLP} models with {CheckList}},
	url = {http://arxiv.org/abs/2005.04118},
	shorttitle = {Beyond Accuracy},
	abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of {NLP} models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce {CheckList}, a task-agnostic methodology for testing {NLP} models. {CheckList} includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of {CheckList} with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, {NLP} practitioners with {CheckList} created twice as many tests, and found almost three times as many bugs as users without it.},
	journaltitle = {{arXiv}:2005.04118 [cs]},
	author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
	urldate = {2020-08-13},
	date = {2020-05-08},
	eprinttype = {arxiv},
	eprint = {2005.04118},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{ribeiro_beyond_2020-1,
	location = {Online},
	title = {Beyond Accuracy: Behavioral Testing of {NLP} Models with {CheckList}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.442},
	doi = {10.18653/v1/2020.acl-main.442},
	shorttitle = {Beyond Accuracy},
	abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of {NLP} models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce {CheckList}, a task-agnostic methodology for testing {NLP} models. {CheckList} includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of {CheckList} with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, {NLP} practitioners with {CheckList} created twice as many tests, and found almost three times as many bugs as users without it.},
	eventtitle = {{ACL} 2020},
	pages = {4902--4912},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
	urldate = {2021-04-07},
	date = {2020-07},
}

@inproceedings{rios_few-shot_2018,
	location = {Brussels, Belgium},
	title = {Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces},
	url = {https://www.aclweb.org/anthology/D18-1352},
	doi = {10.18653/v1/D18-1352},
	abstract = {Large multi-label datasets contain labels that occur thousands of times (frequent group), those that occur only a few times (few-shot group), and labels that never appear in the training dataset (zero-shot group). Multi-label few- and zero-shot label prediction is mostly unexplored on datasets with large label spaces, especially for text classification. In this paper, we perform a fine-grained evaluation to understand how state-of-the-art methods perform on infrequent labels. Furthermore, we develop few- and zero-shot methods for multi-label text classification when there is a known structure over the label space, and evaluate them on two publicly available medical text datasets: {MIMIC} {II} and {MIMIC} {III}. For few-shot labels we achieve improvements of 6.2\% and 4.8\% in R@10 for {MIMIC} {II} and {MIMIC} {III}, respectively, over prior efforts; the corresponding R@10 improvements for zero-shot labels are 17.3\% and 19\%.},
	eventtitle = {{EMNLP} 2018},
	pages = {3132--3142},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Rios, Anthony and Kavuluru, Ramakanth},
	urldate = {2021-06-04},
	date = {2018-10},
}

@inproceedings{ruder_transfer_2019,
	location = {Minneapolis, Minnesota},
	title = {Transfer Learning in Natural Language Processing},
	url = {https://www.aclweb.org/anthology/N19-5004},
	doi = {10.18653/v1/N19-5004},
	abstract = {The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing ({NLP}) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of {NLP} tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and {ImageNet} pretraining in computer vision, and indicate that these methods will likely become a common tool in the {NLP} landscape as well as an important research direction. We will present an overview of modern transfer learning methods in {NLP}, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream {NLP} tasks.},
	pages = {15--18},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
	publisher = {Association for Computational Linguistics},
	author = {Ruder, Sebastian and Peters, Matthew E. and Swayamdipta, Swabha and Wolf, Thomas},
	urldate = {2020-03-31},
	date = {2019-06},
}

@article{ruffy_state_2019,
	title = {The State of Knowledge Distillation for Classification},
	url = {https://arxiv.org/abs/1912.10850v1},
	abstract = {We survey various knowledge distillation ({KD}) strategies for simple
classification tasks and implement a set of techniques that claim
state-of-the-art accuracy. Our experiments using standardized model
architectures, fixed compute budgets, and consistent training schedules
indicate that many of these distillation results are hard to reproduce. This is
especially apparent with methods using some form of feature distillation.
Further examination reveals a lack of generalizability where these techniques
may only succeed for specific architectures and training settings. We observe
that appropriately tuned classical distillation in combination with a data
augmentation training scheme gives an orthogonal improvement over other
techniques. We validate this approach and open-source our code.},
	author = {Ruffy, Fabian and Chahal, Karanbir},
	urldate = {2020-03-13},
	date = {2019-12-20},
	langid = {english},
}

@article{salimans_weight_2016,
	title = {Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},
	url = {http://arxiv.org/abs/1602.07868},
	shorttitle = {Weight Normalization},
	abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as {LSTMs} and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
	journaltitle = {{arXiv}:1602.07868 [cs]},
	author = {Salimans, Tim and Kingma, Diederik P.},
	urldate = {2020-03-06},
	date = {2016-06-03},
	eprinttype = {arxiv},
	eprint = {1602.07868},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	url = {http://arxiv.org/abs/1910.01108},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing ({NLP}), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called {DistilBERT}, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a {BERT} model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	journaltitle = {{arXiv}:1910.01108 [cs]},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	urldate = {2020-03-12},
	date = {2020-02-29},
	eprinttype = {arxiv},
	eprint = {1910.01108},
	keywords = {Computer Science - Computation and Language},
}

@article{schaul_prioritized_2015,
	title = {Prioritized Experience Replay},
	url = {https://arxiv.org/abs/1511.05952v4},
	abstract = {Experience replay lets online reinforcement learning agents remember and
reuse experiences from the past. In prior work, experience transitions were
uniformly sampled from a replay memory. However, this approach simply replays
transitions at the same frequency that they were originally experienced,
regardless of their significance. In this paper we develop a framework for
prioritizing experience, so as to replay important transitions more frequently,
and therefore learn more efficiently. We use prioritized experience replay in
Deep Q-Networks ({DQN}), a reinforcement learning algorithm that achieved
human-level performance across many Atari games. {DQN} with prioritized
experience replay achieves a new state-of-the-art, outperforming {DQN} with
uniform replay on 41 out of 49 games.},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	urldate = {2020-03-17},
	date = {2015-11-18},
	langid = {english},
}

@article{schmidhuber_deep_2015,
	title = {Deep Learning in Neural Networks: An Overview},
	volume = {61},
	issn = {08936080},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10.1016/j.neunet.2014.09.003},
	shorttitle = {Deep Learning in Neural Networks},
	abstract = {In recent years, deep artiﬁcial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	pages = {85--117},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Schmidhuber, Juergen},
	urldate = {2020-04-27},
	date = {2015-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1404.7828},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{schrimpf_neural_2021,
	title = {The neural architecture of language: Integrative modeling converges on predictive processing},
	volume = {118},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2105646118},
	doi = {10.1073/pnas.2105646118},
	shorttitle = {The neural architecture of language},
	abstract = {Significance
            Language is a quintessentially human ability. Research has long probed the functional architecture of language in the mind and brain using diverse neuroimaging, behavioral, and computational modeling approaches. However, adequate neurally-mechanistic accounts of how meaning might be extracted from language are sorely lacking. Here, we report a first step toward addressing this gap by connecting recent artificial neural networks from machine learning to human recordings during language processing. We find that the most powerful models predict neural and behavioral responses across different datasets up to noise levels. Models that perform better at predicting the next word in a sequence also better predict brain measurements—providing computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the brain.
          , 
            The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species’ signature cognitive skill. We find that the most powerful “transformer” models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional {MRI} and electrocorticography). Models’ neural fits (“brain score”) and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.},
	pages = {e2105646118},
	number = {45},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	urldate = {2023-10-29},
	date = {2021-11-09},
	langid = {english},
}

@article{schwarz_progress_2018,
	title = {Progress \& Compress: A scalable framework for continual learning},
	url = {http://arxiv.org/abs/1805.06370},
	shorttitle = {Progress \& Compress},
	abstract = {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress \& compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.},
	journaltitle = {{arXiv}:1805.06370 [cs, stat]},
	author = {Schwarz, Jonathan and Luketina, Jelena and Czarnecki, Wojciech M. and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
	urldate = {2020-10-28},
	date = {2018-07},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{sculley_hidden_2015,
	title = {Hidden Technical Debt in Machine Learning Systems},
	url = {http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf},
	pages = {2503--2511},
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-François and Dennison, Dan},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	urldate = {2019-09-04},
	date = {2015},
}

@article{sculley_machine_2014,
	title = {Machine Learning: The High Interest Credit Card of Technical Debt},
	url = {/paper/Machine-Learning%3A-The-High-Interest-Credit-Card-of-Sculley-Holt/51891710e30da33c4ced4ae7daee1593e0cb5cc4},
	shorttitle = {Machine Learning},
	abstract = {Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns. 1 Machine Learning and Complex Systems Real world software engineers are often faced with the challenge of moving quickly to ship new products or services, which can lead to a dilemma between speed of execution and quality of engineering. The concept of technical debt was first introduced by Ward Cunningham in 1992 as a way to help quantify the cost of such decisions. Like incurring fiscal debt, there are often sound strategic reasons to take on technical debt. Not all debt is necessarily bad, but technical debt does tend to compound. Deferring the work to pay it off results in increasing costs, system brittleness, and reduced rates of innovation. Traditional methods of paying off technical debt include refactoring, increasing coverage of unit tests, deleting dead code, reducing dependencies, tightening {APIs}, and improving documentation [4]. The goal of these activities is not to add new functionality, but to make it easier to add future improvements, be cheaper to maintain, and reduce the likelihood of bugs. One of the basic arguments in this paper is that machine learning packages have all the basic code complexity issues as normal code, but also have a larger system-level complexity that can create hidden debt. Thus, refactoring these libraries, adding better unit tests, and associated activity is time well spent but does not necessarily address debt at a systems level. In this paper, we focus on the system-level interaction between machine learning code and larger systems as an area where hidden technical debt may rapidly accumulate. At a system-level, a machine learning model may subtly erode abstraction boundaries. It may be tempting to re-use input signals in ways that create unintended tight coupling of otherwise disjoint systems. Machine learning packages may often be treated as black boxes, resulting in large masses of “glue code” or calibration layers that can lock in assumptions. Changes in the external world may make models or input signals change behavior in unintended ways, ratcheting up maintenance cost and the burden of any debt. Even monitoring that the system as a whole is operating as intended may be difficult without careful design.},
	journaltitle = {undefined},
	author = {Sculley, D. and Holt, Gary and Golovin, D. and Davydov, Eugene and Phillips, Todd and Ebner, D. and Chaudhary, Vinay and Young, M.},
	urldate = {2021-06-18},
	date = {2014},
	langid = {english},
}

@article{sculley_machine_2014-1,
	title = {Machine Learning: The High Interest Credit Card of Technical Debt},
	url = {/paper/Machine-Learning%3A-The-High-Interest-Credit-Card-of-Sculley-Holt/51891710e30da33c4ced4ae7daee1593e0cb5cc4},
	shorttitle = {Machine Learning},
	abstract = {Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns. 1 Machine Learning and Complex Systems Real world software engineers are often faced with the challenge of moving quickly to ship new products or services, which can lead to a dilemma between speed of execution and quality of engineering. The concept of technical debt was first introduced by Ward Cunningham in 1992 as a way to help quantify the cost of such decisions. Like incurring fiscal debt, there are often sound strategic reasons to take on technical debt. Not all debt is necessarily bad, but technical debt does tend to compound. Deferring the work to pay it off results in increasing costs, system brittleness, and reduced rates of innovation. Traditional methods of paying off technical debt include refactoring, increasing coverage of unit tests, deleting dead code, reducing dependencies, tightening {APIs}, and improving documentation [4]. The goal of these activities is not to add new functionality, but to make it easier to add future improvements, be cheaper to maintain, and reduce the likelihood of bugs. One of the basic arguments in this paper is that machine learning packages have all the basic code complexity issues as normal code, but also have a larger system-level complexity that can create hidden debt. Thus, refactoring these libraries, adding better unit tests, and associated activity is time well spent but does not necessarily address debt at a systems level. In this paper, we focus on the system-level interaction between machine learning code and larger systems as an area where hidden technical debt may rapidly accumulate. At a system-level, a machine learning model may subtly erode abstraction boundaries. It may be tempting to re-use input signals in ways that create unintended tight coupling of otherwise disjoint systems. Machine learning packages may often be treated as black boxes, resulting in large masses of “glue code” or calibration layers that can lock in assumptions. Changes in the external world may make models or input signals change behavior in unintended ways, ratcheting up maintenance cost and the burden of any debt. Even monitoring that the system as a whole is operating as intended may be difficult without careful design.},
	journaltitle = {undefined},
	author = {Sculley, D. and Holt, Gary and Golovin, D. and Davydov, Eugene and Phillips, Todd and Ebner, D. and Chaudhary, Vinay and Young, M.},
	urldate = {2021-06-18},
	date = {2014},
	langid = {english},
}

@inproceedings{sculley_machine_2014-2,
	title = {Machine Learning: The High Interest Credit Card of Technical Debt},
	shorttitle = {Machine Learning},
	booktitle = {{SE}4ML: Software Engineering for Machine Learning ({NIPS} 2014 Workshop)},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
	date = {2014},
}

@book{sejnowski_deep_2018,
	location = {Cambridge, Massachusetts},
	title = {The Deep Learning Revolution},
	isbn = {978-0-262-03803-4},
	abstract = {How deep learning—from Google Translate to driverless cars to personal cognitive assistants—is changing our lives and transforming every sector of the economy.The deep learning revolution has brought us driverless cars, the greatly improved Google Translate, fluent conversations with Siri and Alexa, and enormous profits from automated trading on the New York Stock Exchange. Deep learning networks can play poker better than professional poker players and defeat a world champion at Go. In this book, Terry Sejnowski explains how deep learning went from being an arcane academic field to a disruptive technology in the information economy.Sejnowski played an important role in the founding of deep learning, as one of a small group of researchers in the 1980s who challenged the prevailing logic-and-symbol based version of {AI}. The new version of {AI} Sejnowski and others developed, which became deep learning, is fueled instead by data. Deep networks learn from data in the same way that babies experience the world, starting with fresh eyes and gradually acquiring the skills needed to navigate novel environments. Learning algorithms extract information from raw data; information can be used to create knowledge; knowledge underlies understanding; understanding leads to wisdom. Someday a driverless car will know the road better than you do and drive with more skill; a deep learning network will diagnose your illness; a personal cognitive assistant will augment your puny human brain. It took nature many millions of years to evolve human intelligence; {AI} is on a trajectory measured in decades. Sejnowski prepares us for a deep learning future.},
	pagetotal = {352},
	author = {Sejnowski, Terrence J.},
	date = {2018-10-23},
}

@article{shaw_self-attention_2018,
	title = {Self-Attention with Relative Position Representations},
	url = {http://arxiv.org/abs/1803.02155},
	abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the {WMT} 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 {BLEU} and 0.3 {BLEU} over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
	journaltitle = {{arXiv}:1803.02155 [cs]},
	author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
	urldate = {2019-10-22},
	date = {2018-03-06},
	eprinttype = {arxiv},
	eprint = {1803.02155},
	keywords = {Computer Science - Computation and Language},
}

@article{shin_continual_2017,
	title = {Continual Learning with Deep Generative Replay},
	url = {http://arxiv.org/abs/1705.08690},
	abstract = {Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.},
	journaltitle = {{arXiv}:1705.08690 [cs]},
	author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
	urldate = {2020-10-26},
	date = {2017-12},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{shoeybi_megatron-lm:_2019,
	title = {Megatron-{LM}: Training Multi-Billion Parameter Language Models Using Model Parallelism},
	url = {https://arxiv.org/abs/1909.08053v3},
	shorttitle = {Megatron-{LM}},
	abstract = {Recent work in unsupervised language modeling demonstrates that training
large neural language models advances the state of the art in Natural Language
Processing applications. However, for very large models, memory constraints
limit the size of models that can be practically trained. Model parallelism
allows us to train larger models, because the parameters can be split across
multiple processors. In this work, we implement a simple, efficient intra-layer
model parallel approach that enables training state of the art transformer
language models with billions of parameters. Our approach does not require a
new compiler or library changes, is orthogonal and complimentary to pipeline
model parallelism, and can be fully implemented with the insertion of a few
communication operations in native {PyTorch}. We illustrate this approach by
converging an 8.3 billion parameter transformer language model using 512 {GPUs},
making it the largest transformer model ever trained at 24x times the size of
{BERT} and 5.6x times the size of {GPT}-2. We sustain up to 15.1 {PetaFLOPs} per
second across the entire application with 76\% scaling efficiency, compared to a
strong single processor baseline that sustains 39 {TeraFLOPs} per second, which
is 30\% of peak {FLOPs}. The model is trained on 174GB of text, requiring 12
{ZettaFLOPs} over 9.2 days to converge. Transferring this language model achieves
state of the art ({SOTA}) results on the {WikiText}103 (10.8 compared to {SOTA}
perplexity of 16.4) and {LAMBADA} (66.5\% compared to {SOTA} accuracy of 63.2\%)
datasets. We release training and evaluation code, as well as the weights of
our smaller portable model, for reproducibility.},
	author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and {LeGresley}, Patrick and Casper, Jared and Catanzaro, Bryan},
	urldate = {2019-10-23},
	date = {2019-09-17},
	langid = {english},
}

@article{sukhbaatar_adaptive_2019,
	title = {Adaptive Attention Span in Transformers},
	url = {http://arxiv.org/abs/1905.07799},
	abstract = {We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.},
	journaltitle = {{arXiv}:1905.07799 [cs, stat]},
	author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
	urldate = {2020-08-14},
	date = {2019-08-08},
	eprinttype = {arxiv},
	eprint = {1905.07799},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sun_how_2020,
	title = {How to Fine-Tune {BERT} for Text Classification?},
	url = {http://arxiv.org/abs/1905.05583},
	abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, {BERT} (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of {BERT} on text classification task and provide a general solution for {BERT} fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.},
	journaltitle = {{arXiv}:1905.05583 [cs]},
	author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
	urldate = {2020-04-29},
	date = {2020-02-05},
	eprinttype = {arxiv},
	eprint = {1905.05583},
	keywords = {Computer Science - Computation and Language},
}

@report{sundareswaran_survey_2020,
	title = {A {SURVEY} {ON} {TOOLS} {USED} {FOR} {MACHINE} {LEARNING}},
	abstract = {In this paper, a brief introduction to Machine Learning and its Tools are studied. In the recent developments, most of the Machine learning tools are more advanced and efficient. The various tools learn the machine by using a training set, which predicts the output correctly and efficiently. Machine Learning is applied in different applications such as Agriculture, Data Quality, Information Retrieval, Financial Market Analysis etc.., In this paper, we have discussed few tools like Scikit learn, Pytorch, Tensor flow, Amazon Machine Learning, {KNIME}, Rapid Miner, Keras, and Shogun with its features and its advantages.},
	author = {Sundareswaran, Veena and Shankari, T and Sowmiya, Senthil and Varsha, Mundhra},
	date = {2020-01-01},
	keywords = {\_tablet},
}

@book{sutton_reinforcement_2018,
	location = {Cambridge, Massachusetts},
	edition = {second edition},
	title = {Reinforcement Learning, second edition: An Introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement Learning, second edition},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including {UCB}, Expected Sarsa, and Double Learning. Part {II} extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part {III} has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including {AlphaGo} and {AlphaGo} Zero, Atari game playing, and {IBM} Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	pagetotal = {552},
	publisher = {Bradford Books},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	date = {2018-11-13},
}

@book{sutton_reinforcement_1998,
	location = {Cambridge, Mass},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-19398-6},
	series = {Adaptive computation and machine learning},
	shorttitle = {Reinforcement learning},
	pagetotal = {322},
	publisher = {{MIT} Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	date = {1998},
	langid = {english},
	keywords = {Reinforcement learning},
}

@online{synced_google_2019,
	title = {Google T5 Explores the Limits of Transfer Learning},
	url = {https://medium.com/syncedreview/google-t5-explores-the-limits-of-transfer-learning-a87afbf2615b},
	abstract = {A Google research team recently published the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer…},
	titleaddon = {Medium},
	author = {Synced},
	urldate = {2020-03-30},
	date = {2019-11-09},
	langid = {english},
	note = {Library Catalog: medium.com},
}

@article{tan_survey_2018,
	title = {A Survey on Deep Transfer Learning},
	url = {http://arxiv.org/abs/1808.01974},
	abstract = {As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.},
	journaltitle = {{arXiv}:1808.01974 [cs, stat]},
	author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	urldate = {2020-10-15},
	date = {2018-08},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tavakoli_prioritizing_2019,
	title = {Prioritizing Starting States for Reinforcement Learning},
	url = {http://arxiv.org/abs/1811.11298},
	abstract = {Online, off-policy reinforcement learning algorithms are able to use an experience memory to remember and replay past experiences. In prior work, this approach was used to stabilize training by breaking the temporal correlations of the updates and avoiding the rapid forgetting of possibly rare experiences. In this work, we propose a conceptually simple framework that uses an experience memory to help exploration by prioritizing the starting states from which the agent starts acting in the environment, importantly, in a fashion that is also compatible with on-policy algorithms. Given the capacity to restart the agent in states corresponding to its past observations, we achieve this objective by (i) enabling the agent to restart in states belonging to significant past experiences (e.g., nearby goals), and (ii) promoting faster coverage of the state space through starting from a more diverse set of states. While, using a good priority measure to identify significant past transitions, we expect case (i) to more considerably help exploration in certain domains (e.g., sparse reward tasks), we hypothesize that case (ii) will generally be beneficial, even without any prioritization. We show empirically that our approach improves learning performance for both off-policy and on-policy deep reinforcement learning methods, with most notable gains in highly sparse reward tasks.},
	journaltitle = {{arXiv}:1811.11298 [cs, stat]},
	author = {Tavakoli, Arash and Levdik, Vitaly and Islam, Riashat and Kormushev, Petar},
	urldate = {2020-03-06},
	date = {2019-01-26},
	eprinttype = {arxiv},
	eprint = {1811.11298},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tsuda_modeling_2020,
	title = {A modeling framework for adaptive lifelong learning with transfer and savings through gating in the prefrontal cortex},
	volume = {117},
	rights = {© 2020 . https://www.pnas.org/site/aboutpnas/licenses.{xhtmlPublished} under the {PNAS} license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/47/29872},
	doi = {10.1073/pnas.2009591117},
	abstract = {The prefrontal cortex encodes and stores numerous, often disparate, schemas and flexibly switches between them. Recent research on artificial neural networks trained by reinforcement learning has made it possible to model fundamental processes underlying schema encoding and storage. Yet how the brain is able to create new schemas while preserving and utilizing old schemas remains unclear. Here we propose a simple neural network framework that incorporates hierarchical gating to model the prefrontal cortex’s ability to flexibly encode and use multiple disparate schemas. We show how gating naturally leads to transfer learning and robust memory savings. We then show how neuropsychological impairments observed in patients with prefrontal damage are mimicked by lesions of our network. Our architecture, which we call {DynaMoE}, provides a fundamental framework for how the prefrontal cortex may handle the abundance of schemas necessary to navigate the real world.},
	pages = {29872--29882},
	number = {47},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Tsuda, Ben and Tye, Kay M. and Siegelmann, Hava T. and Sejnowski, Terrence J.},
	urldate = {2021-05-13},
	date = {2020-11-24},
	langid = {english},
	pmid = {33154155},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {gating, lifelong learning, neural networks, prefrontal cortex, reinforcement learning},
}

@misc{tunstall_efficient_2022,
	title = {Efficient Few-Shot Learning Without Prompts},
	url = {http://arxiv.org/abs/2209.11055},
	doi = {10.48550/arXiv.2209.11055},
	abstract = {Recent few-shot methods, such as parameter-efficient fine-tuning ({PEFT}) and pattern exploiting training ({PET}), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose {SetFit} (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers ({ST}). {SetFit} works by first fine-tuning a pretrained {ST} on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that {SetFit} obtains comparable results with {PEFT} and {PET} techniques, while being an order of magnitude faster to train. We also show that {SetFit} can be applied in multilingual settings by simply switching the {ST} body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .},
	number = {{arXiv}:2209.11055},
	publisher = {{arXiv}},
	author = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
	urldate = {2022-11-04},
	date = {2022-09-22},
	eprinttype = {arxiv},
	eprint = {2209.11055 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{tunstall_efficient_2022-1,
	title = {Efficient Few-Shot Learning Without Prompts},
	url = {http://arxiv.org/abs/2209.11055},
	doi = {10.48550/arXiv.2209.11055},
	abstract = {Recent few-shot methods, such as parameter-efficient fine-tuning ({PEFT}) and pattern exploiting training ({PET}), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose {SetFit} (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers ({ST}). {SetFit} works by first fine-tuning a pretrained {ST} on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that {SetFit} obtains comparable results with {PEFT} and {PET} techniques, while being an order of magnitude faster to train. We also show that {SetFit} can be applied in multilingual settings by simply switching the {ST} body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .},
	number = {{arXiv}:2209.11055},
	publisher = {{arXiv}},
	author = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
	urldate = {2022-11-04},
	date = {2022-09-22},
	eprinttype = {arxiv},
	eprint = {2209.11055 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@book{tversky_mind_2019,
	location = {New York},
	edition = {1st edition},
	title = {Mind in Motion: How Action Shapes Thought},
	isbn = {978-0-465-09306-9},
	shorttitle = {Mind in Motion},
	abstract = {An eminent psychologist offers a major new theory of human cognition: movement, not language, is the foundation of {thoughtWhen} we try to think about how we think, we can't help but think of words. Indeed, some have called language the stuff of thought. But pictures are remembered far better than words, and describing faces, scenes, and events defies words. Anytime you take a shortcut or play chess or basketball or rearrange your furniture in your mind, you've done something remarkable: abstract thinking without words. In Mind in Motion, psychologist Barbara Tversky shows that spatial cognition isn't just a peripheral aspect of thought, but its very foundation, enabling us to draw meaning from our bodies and their actions in the world. Our actions in real space get turned into mental actions on thought, often spouting spontaneously from our bodies as gestures. Spatial thinking underlies creating and using maps, assembling furniture, devising football strategies, designing airports, understanding the flow of people, traffic, water, and ideas. Spatial thinking even underlies the structure and meaning of language: why we say we push ideas forward or tear them apart, why we're feeling up or have grown far apart. Like Thinking, Fast and Slow before it, Mind in Motion gives us a new way to think about how--and where--thinking takes place.},
	pagetotal = {384},
	publisher = {Basic Books},
	author = {Tversky, Barbara},
	date = {2019-05-21},
}

@article{van_de_ven_three_2019,
	title = {Three scenarios for continual learning},
	url = {http://arxiv.org/abs/1904.07734},
	abstract = {Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and–in case it is not–whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted {MNIST} task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.},
	journaltitle = {{arXiv}:1904.07734 [cs, stat]},
	author = {van de Ven, Gido M. and Tolias, Andreas S.},
	urldate = {2020-10-15},
	date = {2019-04},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{van_de_ven_generative_2019,
	title = {Generative replay with feedback connections as a general strategy for continual learning},
	url = {http://arxiv.org/abs/1809.10635},
	abstract = {A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted {MNIST} task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as "soft targets") achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.},
	journaltitle = {{arXiv}:1809.10635 [cs, stat]},
	author = {van de Ven, Gido M. and Tolias, Andreas S.},
	urldate = {2020-10-15},
	date = {2019-04},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{van_de_ven_brain-inspired_2020,
	title = {Brain-inspired replay for continual learning with artificial neural networks},
	volume = {11},
	rights = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-17866-2},
	doi = {10.1038/s41467-020-17866-2},
	abstract = {Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the reactivation of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay can be implemented as ‘generative replay’, which can successfully – and surprisingly efficiently – prevent catastrophic forgetting on toy examples even in a class-incremental learning scenario. However, scaling up generative replay to complicated problems with many tasks or complex inputs is challenging. We propose a new, brain-inspired variant of replay in which internal or hidden representations are replayed that are generated by the network’s own, context-modulated feedback connections. Our method achieves state-of-the-art performance on challenging continual learning benchmarks (e.g., class-incremental learning on {CIFAR}-100) without storing data, and it provides a novel model for replay in the brain.},
	pages = {4069},
	number = {1},
	journaltitle = {Nature Communications},
	author = {van de Ven, Gido M. and Siegelmann, Hava T. and Tolias, Andreas S.},
	urldate = {2020-09-23},
	date = {2020-08-13},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {continual learning},
}

@article{van_hasselt_deep_2015,
	title = {Deep Reinforcement Learning with Double Q-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent {DQN} algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the {DQN} algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	journaltitle = {{arXiv}:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	urldate = {2020-03-06},
	date = {2015-12-08},
	eprinttype = {arxiv},
	eprint = {1509.06461},
	keywords = {Computer Science - Machine Learning},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2019-10-22},
	date = {2017-06-12},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{venkatesan_strategy_2017,
	title = {A Strategy for an Uncompromising Incremental Learner},
	url = {http://arxiv.org/abs/1705.00744},
	abstract = {Multi-class supervised learning systems require the knowledge of the entire range of labels they predict. Often when learnt incrementally, they suffer from catastrophic forgetting. To avoid this, generous leeways have to be made to the philosophy of incremental learning that either forces a part of the machine to not learn, or to retrain the machine again with a selection of the historic data. While these hacks work to various degrees, they do not adhere to the spirit of incremental learning. In this article, we redefine incremental learning with stringent conditions that do not allow for any undesirable relaxations and assumptions. We design a strategy involving generative models and the distillation of dark knowledge as a means of hallucinating data along with appropriate targets from past distributions. We call this technique, phantom sampling.We show that phantom sampling helps avoid catastrophic forgetting during incremental learning. Using an implementation based on deep neural networks, we demonstrate that phantom sampling dramatically avoids catastrophic forgetting. We apply these strategies to competitive multi-class incremental learning of deep neural networks. Using various benchmark datasets and through our strategy, we demonstrate that strict incremental learning could be achieved. We further put our strategy to test on challenging cases, including cross-domain increments and incrementing on a novel label space. We also propose a trivial extension to unbounded-continual learning and identify potential for future development.},
	journaltitle = {{arXiv}:1705.00744 [cs]},
	author = {Venkatesan, Ragav and Venkateswara, Hemanth and Panchanathan, Sethuraman and Li, Baoxin},
	urldate = {2020-11-10},
	date = {2017-07},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{wang_dueling_2016,
	title = {Dueling Network Architectures for Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1511.06581},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, {LSTMs}, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our {RL} agent to outperform the state-of-the-art on the Atari 2600 domain.},
	journaltitle = {{arXiv}:1511.06581 [cs]},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	urldate = {2020-03-06},
	date = {2016-04-05},
	eprinttype = {arxiv},
	eprint = {1511.06581},
	keywords = {Computer Science - Machine Learning},
}

@article{wang_glue_2019,
	title = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
	url = {http://arxiv.org/abs/1804.07461},
	shorttitle = {{GLUE}},
	abstract = {For natural language understanding ({NLU}) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark ({GLUE}), a tool for evaluating and analyzing the performance of models across a diverse range of existing {NLU} tasks. {GLUE} is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of {NLU} models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust {NLU} systems.},
	journaltitle = {{arXiv}:1804.07461 [cs]},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	urldate = {2021-05-21},
	date = {2019-02-22},
	eprinttype = {arxiv},
	eprint = {1804.07461},
	keywords = {Computer Science - Computation and Language},
}

@article{wang_generalizing_2020,
	title = {Generalizing from a Few Examples: A Survey on Few-Shot Learning},
	url = {http://arxiv.org/abs/1904.05046},
	shorttitle = {Generalizing from a Few Examples},
	abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning ({FSL}) is proposed to tackle this problem. Using prior knowledge, {FSL} can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand {FSL}. Starting from a formal definition of {FSL}, we distinguish {FSL} from several relevant machine learning problems. We then point out that the core issue in {FSL} is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize {FSL} methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the {FSL} problem setups, techniques, applications and theories, are also proposed to provide insights for future research.},
	journaltitle = {{arXiv}:1904.05046 [cs]},
	author = {Wang, Yaqing and Yao, Quanming and Kwok, James and Ni, Lionel M.},
	urldate = {2021-06-04},
	date = {2020-03-29},
	eprinttype = {arxiv},
	eprint = {1904.05046},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{wang_cordel_2020,
	location = {Sorrento, Italy},
	title = {{CorDEL}: A Contrastive Deep Learning Approach for Entity Linkage},
	isbn = {978-1-72818-316-9},
	url = {https://ieeexplore.ieee.org/document/9338287/},
	doi = {10.1109/ICDM50108.2020.00171},
	shorttitle = {{CorDEL}},
	abstract = {Entity linkage ({EL}) is a critical problem in data cleaning and integration. In the past several decades, {EL} has typically been done by rule-based systems or traditional machine learning models with hand-curated features, both of which heavily depend on manual human inputs. With the ever-increasing growth of new data, deep learning ({DL}) based approaches have been proposed to alleviate the high cost of {EL} associated with the traditional models. Existing exploration of {DL} models for {EL} strictly follows the well-known twin-network architecture. However, we argue that the twin-network architecture is sub-optimal to {EL}, leading to inherent drawbacks of existing models. In order to address the drawbacks, we propose a novel and generic contrastive {DL} framework for {EL}. The proposed framework is able to capture both syntactic and semantic matching signals and pays attention to subtle but critical differences. Based on the framework, we develop a contrastive {DL} approach for {EL}, {CORDEL}, with a simple yet powerful variant called {CORDEL}-Sum. We evaluate {CORDEL} with extensive experiments conducted on both public benchmark datasets and a real-world dataset. {CORDEL} outperforms previous state-of-the-art models by 5.2\% on public benchmark datasets. Moreover, {CORDEL} yields a 29.4\% improvement over the current best {DL} model on the real-world dataset, while reducing the number of training parameters by 96.8\%.},
	eventtitle = {2020 {IEEE} International Conference on Data Mining ({ICDM})},
	pages = {1322--1327},
	booktitle = {2020 {IEEE} International Conference on Data Mining ({ICDM})},
	publisher = {{IEEE}},
	author = {Wang, Zhengyang and Sisman, Bunyamin and Wei, Hao and Dong, Xin Luna and Ji, Shuiwang},
	urldate = {2022-01-24},
	date = {2020-11},
	langid = {english},
}

@article{wang_machamp_2021,
	title = {Machamp: A Generalized Entity Matching Benchmark},
	url = {http://arxiv.org/abs/2106.08455},
	shorttitle = {Machamp},
	abstract = {Entity Matching ({EM}) refers to the problem of determining whether two different data representations refer to the same real-world entity. It has been a long-standing interest of the data management community and many efforts have been paid in creating benchmark tasks as well as in developing advanced matching techniques. However, existing benchmark tasks for {EM} are limited to the case where the two data collections of entities are structured tables with the same schema. Meanwhile, the data collections for matching could be structured, semi-structured, or unstructured in real-world scenarios of data science. In this paper, we come up with a new research problem -- Generalized Entity Matching to satisfy this requirement and create a benchmark Machamp for it. Machamp consists of seven tasks having diverse characteristics and thus provides good coverage of use cases in real applications. We summarize existing {EM} benchmark tasks for structured tables and conduct a series of processing and cleaning efforts to transform them into matching tasks between tables with different structures. Based on that, we further conduct comprehensive profiling of the proposed benchmark tasks and evaluate popular entity matching approaches on them. With the help of Machamp, it is the first time that researchers can evaluate {EM} techniques between data collections with different structures.},
	journaltitle = {{arXiv}:2106.08455 [cs]},
	author = {Wang, Jin and Li, Yuliang and Hirota, Wataru},
	urldate = {2022-03-19},
	date = {2021-06-15},
	eprinttype = {arxiv},
	eprint = {2106.08455},
	note = {version: 1},
	keywords = {Computer Science - Databases},
}

@report{wang_machamp_2021-1,
	title = {Machamp: A Generalized Entity Matching Benchmark},
	url = {http://arxiv.org/abs/2106.08455},
	shorttitle = {Machamp},
	abstract = {Entity Matching ({EM}) refers to the problem of determining whether two different data representations refer to the same real-world entity. It has been a long-standing interest of the data management community and many efforts have been paid in creating benchmark tasks as well as in developing advanced matching techniques. However, existing benchmark tasks for {EM} are limited to the case where the two data collections of entities are structured tables with the same schema. Meanwhile, the data collections for matching could be structured, semi-structured, or unstructured in real-world scenarios of data science. In this paper, we come up with a new research problem -- Generalized Entity Matching to satisfy this requirement and create a benchmark Machamp for it. Machamp consists of seven tasks having diverse characteristics and thus provides good coverage of use cases in real applications. We summarize existing {EM} benchmark tasks for structured tables and conduct a series of processing and cleaning efforts to transform them into matching tasks between tables with different structures. Based on that, we further conduct comprehensive profiling of the proposed benchmark tasks and evaluate popular entity matching approaches on them. With the help of Machamp, it is the first time that researchers can evaluate {EM} techniques between data collections with different structures.},
	number = {{arXiv}:2106.08455},
	institution = {{arXiv}},
	author = {Wang, Jin and Li, Yuliang and Hirota, Wataru},
	urldate = {2022-06-04},
	date = {2021-06-15},
	eprinttype = {arxiv},
	eprint = {2106.08455 [cs]},
	note = {version: 1
type: article},
	keywords = {Computer Science - Databases, \_tablet},
}

@article{wang_linformer_2020,
	title = {Linformer: Self-Attention with Linear Complexity},
	url = {http://arxiv.org/abs/2006.04768},
	shorttitle = {Linformer},
	abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n{\textasciicircum}2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n{\textasciicircum}2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the {\textbackslash}textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
	journaltitle = {{arXiv}:2006.04768 [cs, stat]},
	author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
	urldate = {2021-04-09},
	date = {2020-06-14},
	eprinttype = {arxiv},
	eprint = {2006.04768},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, \_tablet},
}

@article{wei_nezha_2019,
	title = {{NEZHA}: Neural Contextualized Representation for Chinese Language Understanding},
	url = {http://arxiv.org/abs/1909.00204},
	shorttitle = {{NEZHA}},
	abstract = {The pre-trained language models have achieved great successes in various natural language understanding ({NLU}) tasks due to its capacity to capture the deep contextualized information in text by pre-training on large-scale corpora. In this technical report, we present our practice of pre-training language models named {NEZHA} ({NEural} {contextualiZed} representation for {CHinese} {lAnguage} understanding) on Chinese corpora and finetuning for the Chinese {NLU} tasks. The current version of {NEZHA} is based on {BERT} with a collection of proven improvements, which include Functional Relative Positional Encoding as an effective positional encoding scheme, Whole Word Masking strategy, Mixed Precision Training and the {LAMB} Optimizer in training the models. The experimental results show that {NEZHA} achieves the state-of-the-art performances when finetuned on several representative Chinese tasks, including named entity recognition (People's Daily {NER}), sentence matching ({LCQMC}), Chinese sentiment classification ({ChnSenti}) and natural language inference ({XNLI}).},
	journaltitle = {{arXiv}:1909.00204 [cs]},
	author = {Wei, Junqiu and Ren, Xiaozhe and Li, Xiaoguang and Huang, Wenyong and Liao, Yi and Wang, Yasheng and Lin, Jiashu and Jiang, Xin and Chen, Xiao and Liu, Qun},
	urldate = {2020-03-31},
	date = {2019-09-05},
	eprinttype = {arxiv},
	eprint = {1909.00204},
	keywords = {Computer Science - Computation and Language},
}

@article{wolfram_class_2020,
	title = {A Class of Models with the Potential to Represent Fundamental Physics},
	volume = {29},
	issn = {08912513},
	url = {http://arxiv.org/abs/2004.08210},
	doi = {10.25088/ComplexSystems.29.2.107},
	abstract = {A class of models intended to be as minimal and structureless as possible is introduced. Even in cases with simple rules, rich and complex behavior is found to emerge, and striking correspondences to some important core known features of fundamental physics are seen, suggesting the possibility that the models may provide a new approach to finding a fundamental theory of physics.},
	pages = {107--536},
	number = {2},
	journaltitle = {Complex Systems},
	shortjournal = {{ComplexSystems}},
	author = {Wolfram, Stephen},
	urldate = {2021-04-09},
	date = {2020-06-15},
	eprinttype = {arxiv},
	eprint = {2004.08210},
	keywords = {Computer Science - Discrete Mathematics, General Relativity and Quantum Cosmology, High Energy Physics - Theory, Mathematical Physics},
}

@book{wooldridge_brief_2021,
	location = {New York},
	title = {A Brief History of Artificial Intelligence: What It Is, Where We Are, and Where We Are Going},
	isbn = {978-1-250-77074-5},
	shorttitle = {A Brief History of Artificial Intelligence},
	abstract = {From Oxford's leading {AI} researcher comes a fun and accessible tour through the history and future of one of the most cutting edge and misunderstood field in science: Artificial {IntelligenceThe} somewhat ill-defined long-term aim of {AI} is to build machines that are conscious, self-aware, and sentient; machines capable of the kind of intelligent autonomous action that currently only people are capable of. As an {AI} researcher with 25 years of experience, professor Mike Wooldridge has learned to be obsessively cautious about such claims, while still promoting an intense optimism about the future of the field. There have been genuine scientific breakthroughs that have made {AI} systems possible in the past decade that the founders of the field would have hailed as miraculous. Driverless cars and automated translation tools are just two examples of {AI} technologies that have become a practical, everyday reality in the past few years, and which will have a huge impact on our world.While the dream of conscious machines remains, Professor Wooldridge believes, a distant prospect, the floodgates for {AI} have opened. Wooldridge's A Brief History of Artificial Intelligence is an exciting romp through the history of this groundbreaking field--a one-stop-shop for {AI}'s past, present, and world-changing future.},
	pagetotal = {272},
	author = {Wooldridge, Michael},
	date = {2021},
}

@article{wu_group_2018,
	title = {Group Normalization},
	url = {http://arxiv.org/abs/1803.08494},
	abstract = {Batch Normalization ({BN}) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- {BN}'s error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits {BN}'s usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization ({GN}) as a simple alternative to {BN}. {GN} divides the channels into groups and computes within each group the mean and variance for normalization. {GN}'s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On {ResNet}-50 trained in {ImageNet}, {GN} has 10.6\% lower error than its {BN} counterpart when using a batch size of 2; when using typical batch sizes, {GN} is comparably good with {BN} and outperforms other normalization variants. Moreover, {GN} can be naturally transferred from pre-training to fine-tuning. {GN} can outperform its {BN}-based counterparts for object detection and segmentation in {COCO}, and for video classification in Kinetics, showing that {GN} can effectively replace the powerful {BN} in a variety of tasks. {GN} can be easily implemented by a few lines of code in modern libraries.},
	journaltitle = {{arXiv}:1803.08494 [cs]},
	author = {Wu, Yuxin and He, Kaiming},
	urldate = {2021-05-27},
	date = {2018-06-11},
	eprinttype = {arxiv},
	eprint = {1803.08494},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{wu_incremental_2018,
	title = {Incremental Classifier Learning with Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1802.00853},
	abstract = {In this paper, we address the incremental classifier learning problem, which suffers from catastrophic forgetting. The main reason for catastrophic forgetting is that the past data are not available during learning. Typical approaches keep some exemplars for the past classes and use distillation regularization to retain the classification capability on the past classes and balance the past and new classes. However, there are four main problems with these approaches. First, the loss function is not efficient for classification. Second, there is unbalance problem between the past and new classes. Third, the size of pre-decided exemplars is usually limited and they might not be distinguishable from unseen new classes. Forth, the exemplars may not be allowed to be kept for a long time due to privacy regulations. To address these problems, we propose (a) a new loss function to combine the cross-entropy loss and distillation loss, (b) a simple way to estimate and remove the unbalance between the old and new classes , and (c) using Generative Adversarial Networks ({GANs}) to generate historical data and select representative exemplars during generation. We believe that the data generated by {GANs} have much less privacy issues than real images because {GANs} do not directly copy any real image patches. We evaluate the proposed method on {CIFAR}-100, Flower-102, and {MS}-Celeb-1M-Base datasets and extensive experiments demonstrate the effectiveness of our method.},
	journaltitle = {{arXiv}:1802.00853 [cs]},
	author = {Wu, Yue and Chen, Yinpeng and Wang, Lijuan and Ye, Yuancheng and Liu, Zicheng and Guo, Yandong and Zhang, Zhengyou and Fu, Yun},
	urldate = {2020-11-10},
	date = {2018-02},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wu_lite_2020,
	title = {Lite Transformer with Long-Short Range Attention},
	url = {http://arxiv.org/abs/2004.11886},
	abstract = {Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile {NLP} architecture, Lite Transformer to facilitate deploying mobile {NLP} applications on edge devices. The key primitive is the Long-Short Range Attention ({LSRA}), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M {MACs}), Lite Transformer outperforms transformer on {WMT}'14 English-French by 1.2/1.7 {BLEU}, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 {BLEU} score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M {MACs}. Notably, Lite Transformer outperforms the {AutoML}-based Evolved Transformer by 0.5 higher {BLEU} for the mobile {NLP} setting without the costly architecture search that requires more than 250 {GPU} years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.},
	journaltitle = {{arXiv}:2004.11886 [cs]},
	author = {Wu, Zhanghao and Liu, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
	urldate = {2021-04-09},
	date = {2020-04-24},
	eprinttype = {arxiv},
	eprint = {2004.11886},
	keywords = {Computer Science - Computation and Language},
}

@article{xie_adversarial_nodate,
	title = {Adversarial Examples Improve Image Recognition},
	abstract = {Adversarial examples are commonly viewed as a threat to {ConvNets}. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose {AdvProp}, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overﬁtting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that {AdvProp} improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying {AdvProp} to the latest {EfﬁcientNet}-B7 [28] on {ImageNet}, we achieve signiﬁcant improvements on {ImageNet} (+0.7\%), {ImageNet}-C (+6.5\%), {ImageNet}-A (+7.0\%), {StylizedImageNet} (+4.8\%). With an enhanced {EfﬁcientNet}-B8, our method achieves the state-of-the-art 85.5\% {ImageNet} top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images (∼3000× more than {ImageNet}) and ∼9.4× more parameters. Models are available at https://github.com/tensorflow/tpu/tree/ master/models/official/efficientnet.},
	pages = {10},
	author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan and Le, Quoc V},
	langid = {english},
}

@article{yang_finbert_2020,
	title = {{FinBERT}: A Pretrained Language Model for Financial Communications},
	url = {http://arxiv.org/abs/2006.08097},
	shorttitle = {{FinBERT}},
	abstract = {Contextual pretrained language models, such as {BERT} (Devlin et al., 2019), have made significant breakthrough in various {NLP} tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific {BERT} models, {FinBERT}, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of {FinBERT} over generic domain {BERT} model. The code and pretrained models are available at https://github.com/yya518/{FinBERT}. We hope this will be useful for practitioners and researchers working on financial {NLP} tasks.},
	journaltitle = {{arXiv}:2006.08097 [cs]},
	author = {Yang, Yi and {UY}, Mark Christopher Siy and Huang, Allen},
	urldate = {2021-04-09},
	date = {2020-07-08},
	eprinttype = {arxiv},
	eprint = {2006.08097},
	keywords = {Computer Science - Computation and Language},
}

@article{zaheer_big_2020,
	title = {Big Bird: Transformers for Longer Sequences},
	url = {http://arxiv.org/abs/2007.14062},
	shorttitle = {Big Bird},
	abstract = {Transformers-based models, such as {BERT}, have been one of the most successful deep learning models for {NLP}. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, {BigBird}, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that {BigBird} is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as {CLS}), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, {BigBird} drastically improves performance on various {NLP} tasks such as question answering and summarization. We also propose novel applications to genomics data.},
	journaltitle = {{arXiv}:2007.14062 [cs, stat]},
	author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
	urldate = {2020-08-14},
	date = {2020-07-28},
	eprinttype = {arxiv},
	eprint = {2007.14062},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zenke_continual_2017,
	title = {Continual Learning Through Synaptic Intelligence},
	url = {https://arxiv.org/abs/1703.04200v3},
	abstract = {While deep learning has led to remarkable advances across diverse
applications, it struggles in domains where the data distribution changes over
the course of learning. In stark contrast, biological neural networks
continually adapt to changing domains, possibly by leveraging complex molecular
machinery to solve many tasks simultaneously. In this study, we introduce
intelligent synapses that bring some of this biological complexity into
artificial neural networks. Each synapse accumulates task relevant information
over time, and exploits this information to rapidly store new memories without
forgetting old ones. We evaluate our approach on continual learning of
classification tasks, and show that it dramatically reduces forgetting while
maintaining computational efficiency.},
	author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
	urldate = {2020-10-31},
	date = {2017-03-13},
	langid = {english},
}

@article{zhang_dive_nodate,
	title = {Dive into Deep Learning},
	pages = {1025},
	author = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
	langid = {english},
	keywords = {\_tablet},
}

@inproceedings{zhao_how_2020,
	location = {Online},
	title = {How does {BERT}'s attention change when you fine-tune? An analysis methodology and a case study in negation scope},
	url = {https://www.aclweb.org/anthology/2020.acl-main.429},
	doi = {10.18653/v1/2020.acl-main.429},
	shorttitle = {How does {BERT}'s attention change when you fine-tune?},
	abstract = {Large pretrained language models like {BERT}, after fine-tuning to a downstream task, have achieved high performance on a variety of {NLP} problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. We apply this methodology to test {BERT} and {RoBERTa} on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. We find that after fine-tuning {BERT} and {RoBERTa} on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.},
	eventtitle = {{ACL} 2020},
	pages = {4729--4747},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Yiyun and Bethard, Steven},
	urldate = {2021-03-16},
	date = {2020-07},
}

@article{zhu_freelb_2019,
	title = {{FreeLB}: Enhanced Adversarial Training for Language Understanding},
	url = {http://arxiv.org/abs/1909.11764},
	shorttitle = {{FreeLB}},
	abstract = {Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm - {FreeLB}, that promotes higher robustness and invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the {GLUE} benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of {BERT}-based model from 78.3 to 79.4, and {RoBERTa}-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44\% and 67.75\% on {ARC}-Easy and {ARC}-Challenge. Experiments on {CommonsenseQA} benchmark further demonstrate that {FreeLB} can be generalized and boost the performance of {RoBERTa}-large model on other tasks as well.},
	journaltitle = {{arXiv}:1909.11764 [cs]},
	author = {Zhu, Chen and Cheng, Yu and Gan, Zhe and Sun, Siqi and Goldstein, Tom and Liu, Jingjing},
	urldate = {2020-02-10},
	date = {2019-10-05},
	eprinttype = {arxiv},
	eprint = {1909.11764},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{zhu_transfer_2020,
	title = {Transfer Learning in Deep Reinforcement Learning: A Survey},
	url = {http://arxiv.org/abs/2009.07888},
	shorttitle = {Transfer Learning in Deep Reinforcement Learning},
	abstract = {This paper surveys the field of transfer learning in the problem setting of Reinforcement Learning ({RL}). {RL} has been a key solution to sequential decision-making problems. Along with the fast advances of {RL} in various domains, such as robotics and game-playing, transfer learning arises as an important technique to assist {RL} by leveraging and transferring external expertise to boost the learning process of {RL}. In this survey, we review the central issues of transfer learning in the {RL} domain, providing a systematic categorization of its state-of-the-art techniques. We analyze their goals, methodologies, applications, and the {RL} frameworks under which the transfer learning techniques are approachable. We discuss the relationship between transfer learning and other relevant topics from the {RL} perspective and also explore the potential challenges as well as future development directions for transfer learning in {RL}.},
	journaltitle = {{arXiv}:2009.07888 [cs, stat]},
	author = {Zhu, Zhuangdi and Lin, Kaixiang and Zhou, Jiayu},
	urldate = {2020-10-15},
	date = {2020-09},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhuang_comprehensive_2020,
	title = {A Comprehensive Survey on Transfer Learning},
	url = {http://arxiv.org/abs/1911.02685},
	abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
	journaltitle = {{arXiv}:1911.02685 [cs, stat]},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	urldate = {2020-10-15},
	date = {2020-06},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{noauthor_using_nodate,
	title = {Using Apache Kafka to Drive Cutting-Edge Machine Learning {\textbar} Confluent},
	url = {https://www.confluent.io/blog/using-apache-kafka-drive-cutting-edge-machine-learning},
	urldate = {2019-09-04},
}

@online{noauthor_enforcing_nodate,
	title = {Enforcing python version in setup.py},
	url = {https://stackoverflow.com/questions/19534896/enforcing-python-version-in-setup-py},
	titleaddon = {Stack Overflow},
	urldate = {2020-03-03},
	note = {Library Catalog: stackoverflow.com},
}

@online{noauthor_improvements_2018,
	title = {Improvements in Deep Q Learning: Dueling Double {DQN}, Prioritized Experience Replay, and fixed…},
	url = {https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/},
	shorttitle = {Improvements in Deep Q Learning},
	abstract = {by Thomas Simonini

Improvements in Deep Q Learning: Dueling Double {DQN}, Prioritized Experience
Replay, and fixed Q-targets
{\textgreater} This article is part of Deep Reinforcement Learning Course with Tensorflow ?️.
Check the syllabus here.
[https://simoninithomas.github.io/Deep\_reinforcement\_learning\_Course/]
In our last article about Deep Q Learning with Tensorflow
[https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8]
, we implemented an agent that learns to pla},
	titleaddon = {{freeCodeCamp}.org},
	urldate = {2020-03-17},
	date = {2018-07-06},
	langid = {english},
	note = {Library Catalog: www.freecodecamp.org},
}

@online{noauthor_more_nodate,
	title = {More Efficient {NLP} Model Pre-training with {ELECTRA}},
	url = {http://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html},
	abstract = {Posted by Kevin Clark, Student Researcher and Thang Luong, Senior Research Scientist, Google Research, Brain Team   Recent advances in langu...},
	titleaddon = {Google {AI} Blog},
	urldate = {2020-03-30},
	langid = {english},
	note = {Library Catalog: ai.googleblog.com},
}

@online{noauthor_state_nodate,
	title = {The State of Transfer Learning in {NLP}},
	url = {https://ruder.io/state-of-transfer-learning-in-nlp/},
	urldate = {2020-03-31},
}

@online{noauthor_ganfather_nodate,
	title = {The {GANfather}: The man who’s given machines the gift of imagination},
	url = {https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/},
	shorttitle = {The {GANfather}},
	abstract = {One night in 2014, Ian Goodfellow went drinking to celebrate with a fellow doctoral student who had just graduated. At Les 3 Brasseurs (The Three Brewers), a favorite Montreal watering hole, some friends asked for his help with a thorny project they were working on: a computer that could create photos by itself. Researchers were…},
	titleaddon = {{MIT} Technology Review},
	urldate = {2020-11-04},
	langid = {english},
}

@online{noauthor_explainable_nodate,
	title = {Explainable Artificial Intelligence},
	url = {https://www.darpa.mil/program/explainable-artificial-intelligence},
	urldate = {2021-04-13},
}

@online{noauthor_introduction_nodate,
	title = {Introduction to Information Retrieval},
	url = {https://nlp.stanford.edu/IR-book/},
	urldate = {2021-06-25},
}

@online{noauthor_contrastive_nodate,
	title = {Contrastive entity linkage: Mining variational attributes from large catalogs for entity linkage},
	url = {https://www.amazon.science/publications/contrastive-entity-linkage-mining-variational-attributes-from-large-catalogs-for-entity-linkage},
	shorttitle = {Contrastive entity linkage},
	titleaddon = {Amazon Science},
	urldate = {2022-01-24},
	langid = {english},
}

@online{noauthor_collective_nodate,
	title = {Collective knowledge graph multi-type entity alignment},
	url = {https://www.amazon.science/publications/collective-knowledge-graph-multi-type-entity-alignment},
	titleaddon = {Amazon Science},
	urldate = {2022-01-24},
	langid = {english},
}

@misc{noauthor_notitle_nodate,
}

@misc{noauthor_notitle_nodate-1,
}

@online{noauthor_papers_nodate,
	title = {Papers with Code - Profiling Entity Matching Benchmark Tasks},
	url = {https://paperswithcode.com/paper/profiling-entity-matching-benchmark-tasks},
	abstract = {\#2 best model for Entity Resolution on Amazon-Google (F1 (\%) metric)},
	urldate = {2022-03-19},
	langid = {english},
}

@online{noauthor_red_2021,
	title = {Red Hot: The 2021 Machine Learning, {AI} and Data ({MAD}) Landscape},
	url = {https://mattturck.com/data2021/},
	shorttitle = {Red Hot},
	abstract = {Full resolution version of the landscape image here







It’s been a hot, hot year in the world of data, machine learning and {AI}. 



Just when you thought it couldn’t grow any more explosively, the data/{AI} landscape just did: rapid pace of company creation, exciting new product and project launch},
	titleaddon = {Matt Turck},
	urldate = {2022-06-04},
	date = {2021-09-28},
	langid = {american},
	note = {Section: {AI}},
}

@online{noauthor_220402311_nodate,
	title = {[2204.02311] {PaLM}: Scaling Language Modeling with Pathways},
	url = {https://arxiv.org/abs/2204.02311},
	urldate = {2022-07-05},
}

@online{noauthor_introduction_nodate,
	title = {Introduction to Negotiation: A Strategic Playbook for Becoming a Principled and Persuasive Negotiator - Negotiation Caselets},
	url = {https://www.coursera.org/learn/negotiation/home/week/2},
	shorttitle = {Introduction to Negotiation},
	abstract = {You've got the theory. Now let's use it. I'll show how the pie framework applies to some mini cases, or caselets. The Merger Case considers how the synergy gains from a merger will be shared by the two parties. While this is still a stylized ...},
	titleaddon = {Coursera},
	urldate = {2022-07-17},
	langid = {english},
}

@online{noauthor_being_nodate,
	title = {Being You: A New Science of Consciousness: Seth, Anil: 9781524742874: Amazon.com: Books},
	url = {https://www.amazon.com/Being-You-New-Science-Consciousness/dp/1524742872/ref=tmm_hrd_swatch_0?_encoding=UTF8&qid=1695506152&sr=1-4},
	urldate = {2023-09-23},
}

@online{noauthor_experience_nodate,
	title = {The Experience Machine: How Our Minds Predict and Shape Reality: Clark, Andy: 9781524748456: Amazon.com: Books},
	url = {https://www.amazon.com/Experience-Machine-Minds-Predict-Reality/dp/1524748455/ref=tmm_hrd_swatch_0?_encoding=UTF8&qid=1695506203&sr=1-1},
	urldate = {2023-09-23},
}
